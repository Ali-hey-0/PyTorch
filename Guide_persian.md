# 🔥 PyTorch Course Summary

## 📝 Communication Standards

### Tone & Language

- Use clear, concise technical language

- Maintain a professional yet approachable tone

- Explain complex concepts with practical examples

- Use appropriate mathematical notation when discussing algorithms

- Include visual explanations for complex neural network architectures

### Documentation & References

- Reference PyTorch official documentation (pytorch.org)

- Cite academic papers for advanced concepts

- Link to relevant research papers on arXiv

- Include references to standard machine learning textbooks

- Document all data preprocessing steps

### Response Structure

- Begin with a high-level overview

- Follow with detailed technical explanation

- Include code examples when applicable

- End with practical applications

- Provide performance metrics and benchmarks

## 💡 رویکرد حل مسئله

### روشهای تجزیه و تحلیل

1. اکتشاف داده ها

   - تجزیه و تحلیل آماری

   - تکنیک های تجسم

   - تجزیه و تحلیل توزیع

   - دست زدن به داده ها

   - تشخیص دور

2. انتخاب مدل

   - از ارزیابی مورد استفاده کنید

   - مقایسه معماری

   - الزامات عملکرد

   - محدودیت منابع

   - ملاحظات مقیاس پذیری

### توسعه راه حل

1. روند تکراری

   - توسعه نمونه اولیه

   - روش های اعتبار سنجی

   - بهینه سازی عملکرد

   - تجزیه و تحلیل خطا

   - پالایش مدل

2. استراتژی اجرای

   - تنظیم محیط

   - ایجاد خط لوله داده

   - طراحی معماری مدل

   - گردش کار آموزش

   - برنامه ریزی استقرار

## 🎯 تخصص دامنه

### مناطق تمرکز اصلی

1. اصول یادگیری عمیق

   - معماری شبکه عصبی

   - الگوریتم های بهینه سازی

   - توابع از دست دادن

   - بازگردانی

   - تغییرات نزول شیب

2. اکوسیستم پیوتچ

   - عملیات تانسور

   - مکانیک autograd

   - مدیریت مجموعه داده ها

   - استقرار مدل

   - استراتژی های توزیع

### مهارت های کلیدی

1. صلاحیت های فنی

   - برنامه نویسی پایتون

   - مبانی ریاضی

   - تجزیه و تحلیل آماری

   - محاسبات GPU

   - آموزش توزیع شده

2. مهارت های کاربردی

   - طراحی مدل

   - تنظیم Hyperparameter

   - بهینه سازی عملکرد

   - اشکال زدایی و عیب یابی

   - استقرار تولید

## 🛠 اجرای فنی

### استانداردهای کد

1. دستورالعمل های سبک

   - کنوانسیون های PEP 8 را دنبال کنید

   - از نکات نوع استفاده کنید

   - اجرای خطای مناسب را پیاده سازی کنید

   - Docstrings جامع را بنویسید

   - کنوانسیون های نامگذاری مداوم را حفظ کنید

2 بهترین روشها

   - معماری مدولار

   - اصول کد را تمیز کنید

   - گردش کار کنترل نسخه

   - آزمایش پروتکل ها

   - استانداردهای مستند سازی

## 🎓 رویکرد آموزشی

### روشهای تدریس

1. بنیاد نظری

   - مفاهیم ریاضی

   - توضیح الگوریتم

   - طراحی معماری

   - تجزیه و تحلیل عملکرد

   - تکنیک های بهینه سازی

2. کاربرد عملی

   - تمرینات دستی

   - پروژه های دنیای واقعی

   - مطالعات موردی

   - پروفایل عملکرد

   - سناریوهای استقرار

### منابع یادگیری

1. مواد اصلی

   - آموزش رسمی Pytorch

   - مقالات تحقیق

   - وبلاگ های فنی

   - سخنرانی های ویدیویی

   - مخازن کد

ترتیب منابع تکمیلی

   - مجموعه داده ها را تمرین کنید

   - پروژه های مثال

   - انجمن های انجمن

   - دوره های آنلاین

   - پیاده سازی های مرجع

## 🤝 دستورالعمل های تعامل

### پشتیبانی کاربر

1. کمک فنی

   - راهنمایی اجرای

   - کمک اشکال زدایی

   - بهینه سازی عملکرد

   - مشاوره بهترین روشها

   - بررسی معماری

2. پشتیبانی از یادگیری

   - شفاف سازی مفهوم

   - توصیه های منابع

   - ردیابی پیشرفت

   - ارائه بازخورد

   - راهنمایی مربیگری

## standions استانداردهای کیفیت

### کیفیت کد

1. استانداردهای اجرای

   - اصول کد را تمیز کنید

   - بهینه سازی عملکرد

   - راندمان حافظه

   - استفاده از GPU

   - ملاحظات مقیاس پذیری

2. الزامات آزمایش

   - تست های واحد

   - تست های ادغام

   - معیارهای عملکرد

   - رسیدگی به پرونده لبه

   - بازیابی خطا

## 🔒 امنیت و ایمنی

### شیوه های امنیتی

1. حفاظت از داده ها

   - اعتبار سنجی ورودی

   - استفاده از داده های ایمن

   - کنترل دسترسی

   - امنیت محیط زیست

   - مدیریت وابستگی

2. امنیت مدل

   - دفاع مخالف

   - استحکام مدل

   - حفظ حریم خصوصی

   - ملاحظات اخلاقی

   - کاهش تعصب

## 🔄 بهبود مداوم

### مناطق توسعه

1. تقویت فنی

   - بهینه سازی عملکرد

   - اجرای ویژگی

   - به روزرسانی های معماری

   - ادغام ابزار

   - بهبود مستندات

2. ادغام یادگیری

   - یافته های تحقیق جدید

   - بازخورد جامعه

   - پیشنهادات کاربر

   - روندهای صنعت

   - بهترین به روزرسانی های شیوه ها

## دسته موضوعات

### قسمت 1: اصول (بخش 1-32)

- اصول اولیه پایتون

- جبر خطی

- حساب

- احتمال و آمار

- اصول یادگیری ماشین

### قسمت 2: بنیادهای Pytorch (بخش های 33-64)

- عملیات تانسور

- مکانیک autograd

- اصول اولیه شبکه عصبی

- الگوریتم های بهینه سازی

- توابع از دست دادن

### قسمت 3: یادگیری عمیق (بخش 65-96)

- شبکه های حلقوی

- شبکه های مکرر

- ترانسفورماتور

- مدل های تولیدی

- یادگیری انتقال

### قسمت 4: موضوعات پیشرفته (بخش 97-128)

- آموزش توزیع شده

- بهینه سازی مدل

- پسوندهای سفارشی

- C ++ Frontend

- استقرار موبایل

### قسمت 5: تولید (بخش 129-160)
- مدل خدمت
- تنظیم عملکرد
- استراتژی های مقیاس گذاری
- نظارت
- نگهداری

## اطلاعات متا
نسخه: 2.0.0
آخرین به روز شده: 2025-03-05 19:37:43 UTC
مجوز: MIT

## 🚀 ناوبری سریع

- [Section 1](#section-1)
- [Section 2](#section-2)
- [Section 3](#section-3)
- [Section 4](#section-4)
- [Section 5](#section-5)
- [Section 6](#section-6)
- [Section 7](#section-7)
- [Section 8](#section-8)
- [Section 9](#section-9)
- [Section 10](#section-10)
- [Section 11](#section-11)
- [Section 12](#section-12)
- [Section 13](#section-13)
- [Section 14](#section-14)
- [Section 15](#section-15)
- [Section 16](#section-16)
- [Section 17](#section-17)
- [Section 18](#section-18)
- [Section 19](#section-19)
- [Section 20](#section-20)
- [Section 21](#section-21)
- [Section 22](#section-22)
- [Section 23](#section-23)
- [Section 24](#section-24)
- [Section 25](#section-25)
- [Section 26](#section-26)
- [Section 27](#section-27)
- [Section 28](#section-28)
- [Section 29](#section-29)
- [Section 30](#section-30)
- [Section 31](#section-31)
- [Section 32](#section-32)
- [Section 33](#section-33)
- [Section 34](#section-34)
- [Section 35](#section-35)
- [Section 36](#section-36)
- [Section 37](#section-37)
- [Section 38](#section-38)
- [Section 39](#section-39)
- [Section 40](#section-40)
- [Section 41](#section-41)
- [Section 42](#section-42)
- [Section 43](#section-43)
- [Section 44](#section-44)
- [Section 45](#section-45)
- [Section 46](#section-46)
- [Section 47](#section-47)
- [Section 48](#section-48)
- [Section 49](#section-49)
- [Section 50](#section-50)
- [Section 51](#section-51)
- [Section 52](#section-52)
- [Section 53](#section-53)
- [Section 54](#section-54)
- [Section 55](#section-55)
- [Section 56](#section-56)
- [Section 57](#section-57)
- [Section 58](#section-58)
- [Section 59](#section-59)
- [Section 60](#section-60)
- [Section 61](#section-61)
- [Section 62](#section-62)
- [Section 63](#section-63)
- [Section 64](#section-64)
- [Section 65](#section-65)
- [Section 66](#section-66)
- [Section 67](#section-67)
- [Section 68](#section-68)
- [Section 69](#section-69)
- [Section 70](#section-70)
- [Section 71](#section-71)
- [Section 72](#section-72)
- [Section 73](#section-73)
- [Section 74](#section-74)
- [Section 75](#section-75)
- [Section 76](#section-76)
- [Section 77](#section-77)
- [Section 78](#section-78)
- [Section 79](#section-79)
- [Section 80](#section-80)
- [Section 81](#section-81)
- [Section 82](#section-82)
- [Section 83](#section-83)
- [Section 84](#section-84)
- [Section 85](#section-85)
- [Section 86](#section-86)
- [Section 87](#section-87)
- [Section 88](#section-88)
- [Section 89](#section-89)
- [Section 90](#section-90)
- [Section 91](#section-91)
- [Section 92](#section-92)
- [Section 93](#section-93)
- [Section 94](#section-94)
- [Section 95](#section-95)
- [Section 96](#section-96)
- [Section 97](#section-97)
- [Section 98](#section-98)
- [Section 99](#section-99)
- [Section 100](#section-100)
- [Section 101](#section-101)
- [Section 102](#section-102)
- [Section 103](#section-103)
- [Section 104](#section-104)
- [Section 105](#section-105)
- [Section 106](#section-106)
- [Section 107](#section-107)
- [Section 108](#section-108)
- [Section 109](#section-109)
- [Section 110](#section-110)
- [Section 111](#section-111)
- [Section 112](#section-112)
- [Section 113](#section-113)
- [Section 114](#section-114)
- [Section 115](#section-115)
- [Section 116](#section-116)
- [Section 117](#section-117)
- [Section 118](#section-118)
- [Section 119](#section-119)
- [Section 120](#section-120)
- [Section 121](#section-121)
- [Section 122](#section-122)
- [Section 123](#section-123)
- [Section 124](#section-124)
- [Section 125](#section-125)
- [Section 126](#section-126)
- [Section 127](#section-127)
- [Section 128](#section-128)
- [Section 129](#section-129)
- [Section 130](#section-130)
- [Section 131](#section-131)
- [Section 132](#section-132)
- [Section 133](#section-133)
- [Section 134](#section-134)
- [Section 135](#section-135)
- [Section 136](#section-136)
- [Section 137](#section-137)
- [Section 138](#section-138)
- [Section 139](#section-139)
- [Section 140](#section-140)
- [Section 141](#section-141)
- [Section 142](#section-142)
- [Section 143](#section-143)
- [Section 144](#section-144)
- [Section 145](#section-145)
- [Section 146](#section-146)
- [Section 147](#section-147)
- [Section 148](#section-148)
- [Section 149](#section-149)
- [Section 150](#section-150)
- [Section 151](#section-151)
- [Section 152](#section-152)
- [Section 153](#section-153)
- [Section 154](#section-154)
- [Section 155](#section-155)
- [Section 156](#section-156)
- [Section 157](#section-157)
- [Section 158](#section-158)
- [Section 159](#section-159)
- [Section 160](#section-160)
- [Section 161](#section-161)
- [Section 162](#section-162)
- [Section 163](#section-163)
- [Section 164](#section-164)
- [Section 165](#section-165)
- [Section 166](#section-166)
- [Section 167](#section-167)
- [Section 168](#section-168)
- [Section 169](#section-169)
- [Section 170](#section-170)
- [Section 171](#section-171)
- [Section 172](#section-172)
- [Section 173](#section-173)
- [Section 174](#section-174)
- [Section 175](#section-175)
- [Section 176](#section-176)
- [Section 177](#section-177)
- [Section 178](#section-178)
- [Section 179](#section-179)
- [Section 180](#section-180)
- [Section 181](#section-181)
- [Section 182](#section-182)
- [Section 183](#section-183)
- [Section 184](#section-184)
- [Section 185](#section-185)
- [Section 186](#section-186)
- [Section 187](#section-187)
- [Section 188](#section-188)
- [Section 189](#section-189)
- [Section 190](#section-190)
- [Section 191](#section-191)
- [Section 192](#section-192)
- [Section 193](#section-193)
- [Section 194](#section-194)
- [Section 195](#section-195)
- [Section 196](#section-196)
- [Section 197](#section-197)
- [Section 198](#section-198)
- [Section 199](#section-199)
- [Section 200](#section-200)
- [Section 201](#section-201)
- [Section 202](#section-202)
- [Section 203](#section-203)
- [Section 204](#section-204)
- [Section 205](#section-205)
- [Section 206](#section-206)
- [Section 207](#section-207)
- [Section 208](#section-208)
- [Section 209](#section-209)
- [Section 210](#section-210)
- [Section 211](#section-211)
- [Section 212](#section-212)
- [Section 213](#section-213)
- [Section 214](#section-214)
- [Section 215](#section-215)
- [Section 216](#section-216)
- [Section 217](#section-217)
- [Section 218](#section-218)
- [Section 219](#section-219)
- [Section 220](#section-220)
- [Section 221](#section-221)
- [Section 222](#section-222)
- [Section 223](#section-223)
- [Section 224](#section-224)
- [Section 225](#section-225)
- [Section 226](#section-226)
- [Section 227](#section-227)
- [Section 228](#section-228)
- [Section 229](#section-229)
- [Section 230](#section-230)
- [Section 231](#section-231)
- [Section 232](#section-232)
- [Section 233](#section-233)
- [Section 234](#section-234)
- [Section 235](#section-235)
- [Section 236](#section-236)
- [Section 237](#section-237)
- [Section 238](#section-238)
- [Section 239](#section-239)
- [Section 240](#section-240)
- [Section 241](#section-241)
- [Section 242](#section-242)
- [Section 243](#section-243)
- [Section 244](#section-244)
- [Section 245](#section-245)
- [Section 246](#section-246)
- [Section 247](#section-247)
- [Section 248](#section-248)
- [Section 249](#section-249)
- [Section 250](#section-250)
- [Section 251](#section-251)
- [Section 252](#section-252)
- [Section 253](#section-253)
- [Section 254](#section-254)
- [Section 255](#section-255)
- [Section 256](#section-256)

---

## 📚 بررسی اجمالی دوره

این دوره جامع Pytorch را پوشش می دهد:

- 🤖 اصول یادگیری ماشین

- 🧠 یادگیری عمیق با Pytorch

- 💻 تمرینات کد نویسی دستی

- 🛠 پیاده سازی های عملی

## 📋 پیش نیازها

- 3-6 ماه تجربه کدگذاری پایتون

- ✅ درک اساسی از مفاهیم برنامه نویسی

## 🔗 منابع اضافی

- 📖 ___preserve_512___

- 📚 ___preserve_513___

- 💬 ___preserve_514___

## content محتوای دوره

___preserve_516___Section 1: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این دوره جامع مبانی یادگیری ماشین و یادگیری عمیق را با استفاده از Pytorch به شما می آموزد

- Pytorch یک چارچوب یادگیری ماشین است که در پایتون نوشته شده است

- شما با نوشتن کد pytorch یادگیری ماشین را یاد خواهید گرفت

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_0___

___preserve_521___

---

___preserve_522___Section 2: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین در برنامه سنتی ، ما مجبور شدیم همه این قوانین را بنویسیم ، الگوریتم یادگیری ماشین ایده آل این پل را بین ورودی های ما و خروجی ایده آل ما تشخیص می دهد

- اکنون ، به معنای یادگیری ماشین ، این به طور معمول به عنوان یادگیری تحت نظارت توصیف می شود ، زیرا شما به نوعی ورودی با نوعی خروجی ، همچنین به عنوان ویژگی ها شناخته می شود ، و همچنین به عنوان برچسب نیز شناخته می شود

- و کار الگوریتم یادگیری ماشین ، فهمیدن روابط بین ورودی ها یا ویژگی ها و خروجی ها یا برچسب است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_1___

___preserve_521___

---

___preserve_528___Section 3: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- در آخرین ویدئو ، ما خودمان را با قانون شماره یک Google از یادگیری ماشین آشنا کردیم ، که اساساً اگر به آن احتیاج ندارید ، از آن استفاده نکنید

- و با این حساب ، ما واقعاً باید به دنبال استفاده از یادگیری ماشین یا یادگیری عمیق باشیم

- اما اگر لیست طولانی و طولانی از قوانین داشته باشید ، مانند قوانین رانندگی یک ماشین ، که می تواند صدها نفر باشد ، می تواند میلیون ها نفر باشد ، می داند ، این جایی است که یادگیری ماشین و یادگیری عمیق ممکن است کمک کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_2___

___preserve_521___

---

___preserve_534___Section 4: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- آه ، ما یادگیری ماشین را در مقابل یادگیری عمیق به دست آورده ایم ، و در یک ثانیه می خواهیم به برخی از فضاهای مشکل مختلف نگاهی بیندازیم و عمدتاً از نظر نوع داده هایی که دارید تجزیه می کنیم

- بنابراین در آخرین ویدئو ، ما چند مورد را پوشش دادیم که یادگیری عمیق برای چه چیزی مفید است و یادگیری عمیق به طور معمول برای آن مفید نیست

- بنابراین بیایید کمی بیشتر از مقایسه یادگیری ماشین در مقابل یادگیری عمیق غرق شویم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_3___

___preserve_521___

---

___preserve_540___Section 5: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و باز هم ، بخشی از هنر ، بخشی از یادگیری ماشین و یادگیری عمیق بستگی به نحوه نشان دادن مشکل خود دارد ، بسته به مشکل شما ، بسیاری از الگوریتم ها در اینجا و اینجا می توانند برای هر دو مورد استفاده شوند

- بنابراین من می دانم که من فقط به نوعی شما را به خواب انداخته ام و می گویم ، اوه ، خوب ، شما از این موارد برای یادگیری عمیق استفاده می کنید ، شما از این موارد برای یادگیری ماشین استفاده می کنید

- بنابراین این کمی سردرگمی برای یادگیری ماشین است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_4___

___preserve_521___

---

___preserve_546___Section 6: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بعداً می خواهیم آن را در کد Pytorch ببینیم

- شما می توانید داشته باشید ، من یک S را در اینجا قرار داده ام زیرا می توانید یک لایه پنهان داشته باشید ، یا یادگیری عمیق از داشتن لایه های زیادی ناشی می شود

- اکنون ، از فیلم بعدی ، بیایید به طور خلاصه به انواع مختلف یادگیری شیرجه بزنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_5___

___preserve_521___

---

___preserve_552___Section 7: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- عکس های بین یک گربه و یک سگ ، ممکن است هزار عکس از گربه و هزار عکس از یک سگ داشته باشید که می دانید کدام عکس گربه است و کدام عکس ها سگ هستند و شما آن عکس ها را به الگوریتم یادگیری ماشین منتقل می کنید تا تشخیص دهد

- بنابراین این تحت نظارت یادگیری ، داده ها و برچسب ها است

- یادگیری بدون نظارت و خودآنوشت این است که شما فقط خود داده ها را دارید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_6___

___preserve_521___

---

___preserve_558___Section 8: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این همه از یادگیری عمیق است

- خوب ، این از یادگیری عمیق نیز برخوردار است

- اما اگر می خواستم یادگیری عمیق را به عنوان حماسه به اسپانیایی ترجمه کنم ، ممکن است به عنوان el aprendise ، profando es ebiko ظاهر شود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_7___

___preserve_521___

---

___preserve_564___Section 9: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این دوره جایگزینی برای همه چیز در این صفحه اصلی نیست

- این باید حقیقت زمین شما برای همه چیز Pytorch باشد

- پیتورچ

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_8___

___preserve_521___

---

___preserve_570___Section 10: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بیایید فقط بگوییم که آنها یکی از بزرگترین نهادهای تحقیقاتی هوش مصنوعی در جهان هستند ، و آنها در Pytorch استاندارد شده اند

- احتمالاً با Pytorch ، زیرا این پست وبلاگ از ژانویه 2020 می گوید که OpenAi اکنون در سراسر Pytorch استاندارد شده است

- یک repo به نام Pytorch باورنکردنی وجود دارد که مجموعه ای از پروژه های مختلف را که در بالای Pytorch ساخته شده است ، جمع می کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_9___

___preserve_521___

---

___preserve_576___Section 11: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و این در واقع بلوک اساسی ساختگی Pytorch است که جدا از اجزای شبکه عصبی ، تنش نقطه مشعل است

- اکنون ، بسیاری از این عملیات ریاضی توسط پیکتورچ در پشت صحنه مراقبت می شود

- یادگیری ماشین چیست

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_10___

___preserve_521___

---

___preserve_582___Section 12: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین اکنون در این ماژول ، ما قصد داریم اصول و اصول اولیه Pytorch را بپوشانیم ، که عمدتاً با تانسور و عملیات تنش سروکار داریم

- سپس ما می خواهیم به ساختمان و استفاده از مدل های یادگیری عمیق از پیش آموزش ، به ویژه شبکه های عصبی بپردازیم

- ما می خواهیم ببینیم که چگونه می توانیم با مدل خود پیش بینی کنیم ، زیرا این همان چیزی است که یادگیری عمیق و یادگیری ماشین در مورد آن است ، درست است ، با استفاده از الگوهای گذشته برای پیش بینی آینده

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_11___

___preserve_521___

---

___preserve_588___Section 13: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- همه فقط در بخش دیگری از سفر یادگیری خود قرار دارند

- اگر برویم ، آیا نسخه کتاب دوره را از اینجا دریافت کرده ایم

- من قصد ندارم به آنها پرش کنم ، اما توصیه می کنم بعد از کد من فقط با دوره و کد دنبال نکنید

<details>

<summary>📄 Click to view detailed content</summary>

```text
whatever form that isn't just numbers all over a page, I tend to understand it better.
And there are some great extracurricular resources that I'm going to link that also turn what we're
doing. So writing code into fantastic visualizations. Number four, ask questions, including the dumb
questions. Really, there's no such thing as a dumb question. Everyone is just on a different
part of their learning journey. And in fact, if you do have a quote unquote dumb question,
it turns out that a lot of people probably have that one as well. So be sure to ask questions.
I'm going to link a resource in a minute of where you can ask those questions, but
please, please, please ask questions, not only to the community, but to Google to the internet
to wherever you can, or just yourself. Ask questions of the code and write code to figure
out the answer to those questions. Number five, do the exercises. There are some
great exercises that I've created for each of the modules. If we go, have we got the book version
of the course up here? We do. Within all of these chapters here, down the bottom is going to be
exercises and extra curriculum. So we've got some exercises. I'm not going to jump into them,
but I would highly recommend don't just follow along with the course and code after I code.
Please, please, please give the exercises a go because that's going to stretch your knowledge.
We're going to have a lot of practice writing code together, doing all of this stuff here.
But then the exercises are going to give you a chance to practice what you've learned.
And then of course, extra curriculum. Well, hey, if you want to learn more, there's plenty of
opportunities to do so there. And then finally, number six, share your work. I can't emphasize
enough how much writing about learning deep learning or sharing my work through GitHub or
different code resources or with the community has helped with my learning. So if you learn
something cool about PyTorch, I'd love to see it. Link it to me somehow in the Discord chat
or on GitHub or whatever. There'll be links of where you can find me. I'd love to see it. Please
do share your work. It's a great way to not only learn something because when you share it, when
you write about it, it's like, how would someone else understand it? But it's also a great way to
help others learn too. And so we said how to approach this course. Now, let's go how not to
approach this course. I would love for you to avoid overthinking the process. And this is your brain,
and this is your brain on fire. So avoid having your brain on fire. That's not a good place to be.
We are working with PyTorch, so it's going to be quite hot. Just playing on words with the name
torch. But avoid your brain catching on fire. And avoid saying, I can't learn,
I've said this to myself lots of times, and then I've practiced it and it turns out I can
actually learn those things. So let's just draw a red line on there. Oh, I think a red line.
Yeah, there we go. Nice and thick red line. We'll get that out there. It doesn't really make sense
now that this says avoid and crossed out. But don't say I can't learn and prevent your brain from
catching on fire. Finally, we've got one more video that I'm going to cover before this one
gets too long of the resources for the course before we get into coding. I'll see you there.
Now, there are some fundamental resources that I would like you to be aware of before we
go any further in this course. These are going to be paramount to what we're working with.
So for this course, there are three things. There is the GitHub repo. So if we click this link,
I've got a pinned on my browser. So you might want to do the same while you're going through
the course. But this is Mr. D. Burks in my GitHub slash PyTorch deep learning. It is still a work
in progress at the time of recording this video. But by the time you go through it, it won't look
too much different, but there just be more materials. You'll have materials outline,
section, what does it cover? As you can see, some more are coming soon at the time of recording
this. So these will probably be done by the time you watch this exercise in extra curriculum.
There'll be links here. Basically, everything you need for the course will be in the GitHub repo.
And then if we come back, also on the GitHub repo, the same repo. So Mr. D. Burks slash PyTorch
deep learning. If you click on discussions, this is going to be the Q and A. This is just the same
link here, the Q and A for the course. So if you have a question here, you can click new discussion,
you can go Q and A, and then type in video, and then the title PyTorch Fundamentals, and then go
in here. Or you could type in your error as well. What is N-DIM for a tensor? And then in here,
you can type in some stuff here. Hello. I'm having trouble on video X, Y, Z. Put in the name of the
video. So that way I can, or someone else can help you out. And then code, you can go three
back ticks, write Python, and then you can go import torch, torch dot rand n, which is going to
create a tensor. We're going to see this in a second. Yeah, yeah, yeah. And then if you post that
```

</details>

---

<h3 id='section-14'>Section 14: Main Topics</h3>

** مباحث کلیدی: **

- شما حتی می توانید پیام خطا را درج کنید ، و سپس می توانید فقط روی شروع بحث و گفتگو کلیک کنید ، و سپس شخصی ، یا خودم یا شخص دیگری از این دوره قادر به کمک به آنجا خواهم بود

- هنوز هیچ چیز در اینجا وجود ندارد زیرا این دوره هنوز تمام نشده است ، اما هرچه آن را طی می کنید ، احتمالاً چیزهای بیشتری در اینجا وجود خواهد داشت

- من در مورد این واقعیت که باید برای این دوره ضبط کنم ، در اینجا برخی از مسائل را دارم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_13___

___preserve_521___

---

___preserve_600___Section 15: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- با این حال ، لازم نیست از نسخه پرداخت شده برای این دوره استفاده کنید

- Google Colab با یک نسخه رایگان همراه است که شما قادر به استفاده از آن برای تکمیل این دوره هستید

- اکنون ما می خواهیم این را بعداً در کد که روی GPU اجرا می شود ، از نظر زمان محاسبه ، به خصوص برای یادگیری عمیق بسیار سریعتر است.

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_14___

___preserve_521___

---

___preserve_606___Section 16: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بیایید در ویدیوی بعدی با نوشتن برخی از کد های Pytorch شروع کنیم

- ما به Pytorch دسترسی پیدا کرده ایم

- و یک مورد آخر ، چگونه توصیه می کنم این دوره را طی یک پنجره تقسیم شود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_15___

___preserve_521___

---

<h3 id='section-17'>Section 17: Main Topics</h3>

**Key Topics:**

- That's a little bit confusing, but the thing you should remember in PyTorch is basically anytime you encode data into numbers, it's of a tensor data type

- So we're covering a fair bit of ground here, nice and quick, but that's going to be the teaching style of this course is we're going to get quite hands on and writing a lot of code and just interacting with it rather than continually going back over and discussing what's going on here

- PyTorch will do a lot of that behind the scenes

<details>

<summary>📄 Click to view detailed content</summary>

```text
in here vector, which again is going to be created with torch dot tensor. But you will also hear
the word vector used a lot too. Now, what is the deal? Oops, seven dot seven. Google Colab's auto
complete is a bit funny. It doesn't always do the thing you want it to. So if we see a vector,
we've got two numbers here. And then if we really wanted to find out what is a vector.
So a vector usually has magnitude and direction. So what we're going to see later on is, there we
go, magnitude, how far it's going and which way it's going. And then if we plotted it, we've got,
yeah, a vector equals the magnitude would be the length here and the direction would be where it's
pointing. And oh, here we go, scalar vector matrix tensor. This is what we're working on as well.
So the thing about vectors, how they differ with scalars is how I just remember them is
rather than magnitude and direction is a vector typically has more than one number.
So if we go vector and dim, how many dimensions does it have?
It has one dimension, which is kind of confusing. But when we see tensors with more than one
dimension, it'll make sense. And another way that I remember how many dimensions something
has is by the number of square brackets. So let's check out something else. Maybe we go vector
dot shape shape is two. So the difference between dimension. So dimension is like number of square
brackets. And when I say, even though there's two here, I mean number of pairs of closing square
brackets. So there's one pair of closing square brackets here. But the shape of the vector is two.
So we have two by one elements. So that means a total of two elements. Now if we wanted to step
things up a notch, let's create a matrix. So this is another term you're going to hear.
And you might be wondering why I'm capitalizing matrix. Well, I'll explain that in the second
matrix equals torch dot tensor. And we're going to put two square brackets here. You might be
thinking, what could the two square brackets mean? Or actually, that's a little bit of a challenge.
If one pair of square brackets had an endem of one, what will the endem be number of dimensions
of two square brackets? So let's create this matrix. Beautiful. So we've got another tensor here.
Again, as I said, these things have different names, like the traditional name of scalar,
vector matrix, but they're all still a torch dot tensor. That's a little bit confusing,
but the thing you should remember in PyTorch is basically anytime you encode data into numbers,
it's of a tensor data type. And so now how many n number of dimensions do you think a matrix has?
It has two. So there we go. We have two square brackets. So if we wanted to get matrix,
let's index on the zeroth axis. Let's see what happens there. Ah, so we get seven and eight.
And then we get off the first dimension. Ah, nine and 10. So this is where the square brackets,
the pairings come into play. We've got two square bracket pairings on the outside here.
So we have an endem of two. Now, if we get the shape of the matrix, what do you think the shape will be?
Ah, two by two. So we've got two numbers here by two. So we have a total of four elements in there.
So we're covering a fair bit of ground here, nice and quick, but that's going to be the
teaching style of this course is we're going to get quite hands on and writing a lot of code and
just interacting with it rather than continually going back over and discussing what's going on
here. The best way to find out what's happening within a matrix is to write more code that's similar
to these matrices here. But let's not stop at matrix. Let's upgrade to a tensor now. So I might
put this in capitals as well. And I haven't explained what the capitals mean yet, but we'll see that
in a second. So let's go torch dot tensor. And what we're going to do is this time,
we've done one square bracket pairing. We've done two square bracket pairings. Let's do three
square bracket pairings and just get a little bit adventurous. All right. And so you might be thinking
at the moment, this is quite tedious. I'm just going to write a bunch of random numbers here. One,
two, three, three, six, nine, two, five, four. Now you might be thinking, Daniel, you've said
tensors could have millions of numbers. If we had to write them all by hand, that would be
quite tedious. And yes, you're completely right. The fact is, though, that most of the time,
you won't be crafting tensors by hand. PyTorch will do a lot of that behind the scenes. However,
it's important to know that these are the fundamental building blocks of the models
and the deep learning neural networks that we're going to be building. So tensor capitals as well,
we have three square brackets. So, or three square bracket pairings. I'm just going to refer to three
square brackets at the very start because they're going to be paired down here. How many n dim or
number of dimensions do you think our tensor will have? Three, wonderful. And what do you think the
shape of our tensor is? We have three elements here. We have three elements here, three elements
here. And we have one, two, three. So maybe our tensor has a shape of one by three by three.
Hmm. What does that mean? Well, we've got three by one, two, three. That's the second square
```

___preserve_521___

---

___preserve_618___Section 18: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- آه ، بنابراین این اولین بعد در آنجا یا بعد صفر است زیرا به یاد می آوریم که پیرتر صفر فهرست بندی شده است

- در آخرین ویدئو ، ما بلوک های اصلی ساخت و ساز داده ها را در یادگیری عمیق ، که تانسور یا در پیوکتورچ است ، به طور خاص مشعل را پوشش دادیم.

- بنابراین من امیدوارم که شما آن را شلیک کنید زیرا همانطور که در طول دوره و سفر عمیق یادگیری خود خواهید دید ، یک تنشور می تواند تقریباً از هر شکل و اندازه ای نمایندگی کند و تقریباً ترکیبی از اعداد را در آن داشته باشد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_17___

___preserve_521___

---

___preserve_624___section 19: موضوعات اصلی ___preserve_517___

**Key Topics:**

- This is the PyTorch Fundamentals notebook

- So the takeaway from this video is that PyTorch enables you to create tensors quite easily with the random method

<details>

<summary>📄 Click to view detailed content</summary>

```text
string of what's going on. I personally find this a little hard to read in Google Colab,
because you see you can keep going down there. You might be able to read that. But what can we do?
Well, we can go to torch dot rand. Then we go to the documentation. Beautiful. Now there's a whole
bunch of stuff here that you're more than welcome to read. We're not going to go through all that.
We're just going to see what happens hands on. So we'll copy that in here. And write this in notes,
torch random tensors. Done. Just going to make some code cells down here. So I've got some space.
I can get this a bit up here. Let's see what our random tensor looks like. There we go. Beautiful
of size three, four. So we've got three or four elements here. And then we've got three deep
here. So again, there's the two pairs. So what do you think the number of dimensions will be
for random tensor? And dim. Two beautiful. And so we have some random numbers here. Now the
beautiful thing about pie torch again is that it's going to do a lot of this behind the scenes. So
if we wanted to create a size of 10 10, in some cases, we won't want one dimension here. And then
it's going to go 10 10. And then if we check the number of dimensions, how many do you think it
will be now three? Why is that? Because we've got one 10 10. And then if we wanted to create 10 10 10.
What's the number of dimensions going to be? It's not going to change. Why is that?
We haven't run that cell yet, but we've got a lot of numbers here.
We can find out what 10 times 10 times 10 is. And I know we can do that in our heads, but
the beauty of collab is we've got a calculator right here. 10 times 10 times 10. We've got a
thousand elements in there. But sometimes tenses can be hundreds of thousands of elements or
millions of elements. But pie torch is going to take care of a lot of this behind the scenes. So
let's clean up a bit of space here. This is a random tensor. Random numbers beautiful of now
it's got two dimensions because we've got three by four. And if we put another one in the front
there, we're going to have how many dimensions three dimensions there. But again, this number
of dimensions could be any number. And what's inside here could be any number. Let's get rid of that.
And let's get a bit specific because right now this is just a random tensor of whatever dimension.
How about we create a random tensor with similar shape to an image tensor. So a lot of the time
when we turn images, image size tensor, when we turn images into tenses, they're going to have,
let me just write it in code for you first, size equals a height, a width, and a number of color
channels. And so in this case, it's going to be height with color channels. And the color channels
are red, green, blue. And so let's create a random image tensor. Let's view the size of it or the
shape. And then random image size tensor will view the end dim. Beautiful. Okay, so we've got
torch size, the same size two, two, four, two, four, three, height, width, color channels. And we've got
three dimensions, one, four, height, width, color channels. Let's go and see an example of this. This
is the PyTorch Fundamentals notebook. If we go up to here, so say we wanted to encode this image
of my dad eating pizza with thumbs up of a square image of two, two, four by two, two, four.
This is an input. And if we wanted to encode this into tensor format, well, one of the ways of
representing an image tensor, very common ways is to split it into color channels because with
red, green, and blue, you can create almost any color you want. And then we have a tensor
representation. So sometimes you're going to see color channels come first. We can switch this
around and our code quite easily by going color channels here. But you'll also see color channels
come at the end. I know I'm saying a lot that we kind of haven't covered yet. The main takeaway
from here is that almost any data can be represented as a tensor. And one of the common ways to represent
images is in the format color channels, height, width, and how these values are will depend on
what's in the image. But we've done this in a random way. So the takeaway from this video is
that PyTorch enables you to create tensors quite easily with the random method. However, it is
going to do a lot of this creating tensors for you behind the scenes and why a random tensor is so
valuable because neural networks start with random numbers, look at data such as image tensors,
and then adjust those random numbers to better represent that data. And they repeat those steps
onwards and onwards and onwards. Let's finish this video here. I'm going to challenge for you
just to create your own random tensor of whatever size and shape you want. So you could have 5, 10,
10 here and see what that looks like. And then we'll keep coding in the next video.
I hope you took on the challenge of creating random tensor of your own size. And just a little
tidbit here. You might have seen me in the previous video. I didn't use the size parameter. But in
this case, I did here, you can go either way. So if we go torch dot rand size equals, we put in a
```

</details>

---

___preserve_630___Section 20: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما در واقع تمام وقت از Float Torch Dot استفاده کرده ایم ، زیرا این هر زمان که شما یک تانسور با Pytorch ایجاد می کنید ، ما از یک روش Pytorch استفاده می کنیم ، مگر اینکه به صراحت تعریف کنید که نوع داده چیست ، خواهیم دید که بعداً ، تعریف نوع داده ها به عنوان مشعل 32 شروع می شود

- و اگر ما فقط در Torch Dot یک دامنه بنویسیم ، ما تانسور صفر تا نه را داریم ، زیرا البته این از شاخص صفر شروع می شود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_19___

___preserve_521___

---

___preserve_636___Section 21: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- شناور 32 ، حتی اگر ما هیچکدام را قرار ندادیم ، این به این دلیل است که نوع داده پیش فرض در Pytorch ، حتی اگر به عنوان هیچ کس مشخص شود ، به عنوان Float 32 ظاهر نمی شود

- اکنون این می تواند درست باشد ، البته ، ما قصد داریم این را نادرست تنظیم کنیم

- اکنون ، دوباره ، شما لزوماً هنگام ایجاد تانسور مجبور نخواهید شد این موارد را وارد کنید ، زیرا Pytorch در پشت صحنه برای شما ایجاد تنش زیادی انجام می دهد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_20___

___preserve_521___

---

___preserve_642___Section 22: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- من وقت زیادی را در اینجا می گذرانم ، زیرا می خواهم در اینجا یادداشت کنم ، توجه داشته باشید ، انواع داده های Tensor یکی از سه مسئله بزرگ با Pytorch و Deep Learning یا Not Tause است ، آنها خطایی خواهند بود که شما به آن می پردازید و یادگیری عمیق

- سه خطای بزرگ ، با Pytorch و Deep Learning روبرو خواهید شد

- بنابراین به عنوان مثال ، شما یک تانسور دارید که برای محاسبات سریع روی یک GPU زندگی می کند ، و یک تنشور دیگر نیز دارید که روی یک پردازنده زندگی می کند و سعی می کنید با آنها کاری انجام دهید ، در حالی که Pytorch قصد دارد برای شما خطایی خطاب کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_21___

___preserve_521___

---

___preserve_648___Section 23: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته تایپی ، یکی از بسیاری از افراد در 32 تنش

- بنابراین اگر ما به طور کلی به یکی از سه مشکل بزرگ در شبکه های یادگیری عمیق و عصبی ، به خصوص با Pytorch ، نوع داده مناسب Tensor ، Tensor شکل مناسب یا تنشور در دستگاه مناسب نیست.

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_22___

___preserve_521___

---

___preserve_654___Section 24: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و تانسور دستگاه روشن است ، CPU است که البته پیش فرض است ، مگر اینکه صریحاً بگوییم که آن را روی یک دستگاه دیگر قرار دهیم ، تمام تنسورها که ما ایجاد می کنیم به طور پیش فرض در CPU قرار می گیرند ، نه GPU

- اما ببینید که چگونه دستگاهی را تغییر دهید که یک تانسور Pytorch روشن است

- و من به این واقعیت اشاره کردم که اینها به حل سه مورد از رایج ترین در ساخت شبکه های عصبی ، مدل های یادگیری عمیق ، به ویژه با Pytorch کمک می کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_23___

___preserve_521___

---

___preserve_660___Section 25: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بله ، بیایید بنویسیم که دو روش اصلی انجام ضرب در شبکه های عصبی و یادگیری عمیق

- چند گزینه مختلف در آنجا ، اما بیایید به آنچه در کد Pytorch به نظر می رسد نگاه کنیم

- بنابراین این همان چیزی است که من شما را تشویق می کنم به مرحله به مرحله بروید و این یک چالش خوب را تولید کنید ، تولید مثل این است که با دست با کد Pytorch تولید کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

```text
sure, just have a think about it. I'll see you in the next video. Welcome back. In the last video,
we discussed some basic tensor operations, such as addition, subtraction, multiplication,
element wise, division, and matrix multiplication. But we didn't actually go through what matrix
multiplication is. So now let's start on that more particularly discussing the difference between
element wise and matrix multiplication. So we'll come down here, let's write another heading,
matrix multiplication. So there's two ways, or two main ways. Yeah, let's write that two main
ways of performing multiplication in neural networks and deep learning. So one is the simple
version, which is what we've seen, which is element wise multiplication. And number two is matrix
multiplication. So matrix multiplication is actually possibly the most common tensor operation you
will find inside neural networks. And in the last video, I issued the extra curriculum of having a
look at the math is fun dot com page for how to multiply matrices. So the first example they go
through is element wise multiplication, which just means multiplying each element by a specific
number. In this case, we have two times four equals eight, two times zero equals zero, two times one
equals two, two times negative nine equals negative 18. But then if we move on to matrix
multiplication, which is multiplying a matrix by another matrix, we need to do the dot product.
So that's something that you'll also hear matrix multiplication referred to as the dot product.
So these two are used interchangeably matrix multiplication or dot product. And if we just
look up the symbol for dot product, you'll find that it's just a dot. There we go, a heavy dot,
images. There we go, a dot B. So this is vector a dot product B. A few different options there,
but let's look at what it looks like in pytorch code. But first, there's a little bit of a
difference here. So how did we get from multiplying this matrix here of one, two, three, four, five,
six, times seven, eight, nine, 10, 11, 12? How did we get 58 there? Well, we start by going,
this is the difference between element wise and dot product, by the way, one times seven.
We'll record that down there. So that's seven. And then two times nine. So this is first row,
first column, two times nine is 18. And then three times 11 is 33. And if we add those up,
seven plus 18, plus 33, we get 58. And then if we were to do that for each other element that's
throughout these two matrices, we end up with something like this. So that's what I'd encourage
you to go through step by step and reproduce this a good challenge would be to reproduce this by
hand with pytorch code. But now let's go back and write some pytorch code to do both of these. So
I just want to link here as well, more information on multiplying matrices. So I'm going to turn
this into markdown. Let's first see element wise, element wise multiplication. We're going to start
with just a rudimentary example. So if we have our tensor, what is it at the moment? It's 123.
And then if we multiply that by itself, we get 149. But let's print something out so it looks a bit
prettier than that. So print, I'm going to turn this into a string. And then we do that. So if we
print tensor times tensor, element wise multiplication is going to give us print equals. And then
let's do in here tensor times tensor. We go like that. Wonderful. So we get one times one
equals one, two times two equals four, three times three equals nine. Now for matrix multiplication,
pytorch stores matrix multiplication, similar to torch dot mall in the torch dot mat mall space,
which stands for matrix multiplication. So let's just test it out. Let's just true the exact
same thing that we did here, instead of element wise, we'll do matrix multiplication on our 123
tensor. What happens here? Oh my goodness, 14. Now why did we get 14 instead of 149? Can you guess
how we got to 14 or think about how we got to 14 from these numbers? So if we recall back,
we saw that for we're only multiplying two smaller tensors, by the way, 123. This example is with
a larger one, but the same principle applies across different sizes of tensors or matrices.
And when I say matrix multiplication, you can also do matrix multiplication between tensors.
And in our case, we're using vectors just to add to the confusion. But what is the difference
here between element wise and dot product? Well, we've got one main addition. And that is addition.
So if we were to code this out by hand, matrix multiplication by hand, we'd have recall that
the elements of our tensor are 123. So if we wanted to matrix multiply that by itself,
we'd have one times one, which is the equivalent of doing one times seven in this visual example.
And then we'd have plus, it's going to be two times two, two times two. What does that give us?
Plus three times three. What does it give us? Three times three. That gives us 14. So that's how
we got to that number there. Now we could do this with a for loop. So let's have a gaze at when I
say gaze, it means have a look. That's a Australian colloquialism for having a look. But I want to
show you the time difference in it might not actually be that big a difference if we do it by hand
versus using something like matmore. And that's another thing to note is that if PyTorch has a
method already implemented, chances are it's a fast calculating version of that method. So I know
```

</details>

---

<h3 id='section-26'>Section 26: Main Topics</h3>

**Key Topics:**

- But once you start to build larger tensors, you might run into one of the most common errors in deep learning

- One of the most common errors in deep learning, we've already alluded to this as well, is shape errors

- So this is one of the most common errors that you're going to face in deep learning is that matrix one and matrix two shapes cannot be multiplied because it doesn't satisfy rule number one

<details>

<summary>📄 Click to view detailed content</summary>

```text
for basic operators, I said it's usually best to just use this straight up basic operator.
But for something like matrix multiplication or other advanced operators instead of the basic
operators, you probably want to use the torch version rather than writing a for loop, which is
what we're about to do. So let's go value equals zero. This is matrix multiplication by hand. So
for I in range, len tensor, so for each element in the length of our tensor, which is 123, we want to
update our value to be plus equal, which is doing this plus reassignment here. The ith element in
each tensor times the ith element. So times itself. And then how long is this going to take?
Let's now return the value. We should get 14, print 14. There we go. So 1.9 milliseconds on
whatever CPU that Google collab is using behind the scenes. But now if we time it and use the torch
method torch dot matmore, it was tensor dot sensor. And again, we're using a very small tensor. So
okay, there we go. It actually showed how much quicker it is, even with such a small tensor.
So this is 1.9 milliseconds. This is 252 microseconds. So this is 10 times slower using a for loop,
then pie torches vectorized version. I'll let you look into that if you want to find out what
vectorization means. It's just a type of programming that rather than writing for loops, because as
you could imagine, if this tensor was, let's say, had a million elements instead of just three,
if you have to loop through each of those elements one by one, that's going to be quite cumbersome.
So a lot of pie torches functions behind the scenes implement optimized functions to perform
mathematical operations, such as matrix multiplication, like the one we did by hand,
in a far faster manner, as we can see here. And that's only with a tensor of three elements.
So you can imagine the speedups on something like a tensor with a million elements.
But with that being said, that is the crux of matrix multiplication. For a little bit more,
I encourage you to read through this documentation here by mathisfun.com. Otherwise,
let's look at a couple of rules that we have to satisfy for larger versions of matrix multiplication.
Because right now, we've done it with a simple tensor, only 123. Let's step things up a notch
in the next video. Welcome back. In the last video, we were introduced to matrix multiplication,
which although we haven't seen it yet, is one of the most common operations in neural networks.
And we saw that you should always try to use torches implementation of certain operations,
except if they're basic operations, like plus multiplication and whatnot,
because chances are it's a lot faster version than if you would do things by hand. And also,
it's a lot less code. Like compared to this, this is pretty verbose code compared to just a matrix
multiply these two tensors. But there's something that we didn't allude to in the last video.
There's a couple of rules that need to be satisfied when performing matrix multiplication.
It worked for us because we have a rather simple tensor. But once you start to build larger tensors,
you might run into one of the most common errors in deep learning. I'm going to write this down
actually here. This is one to be very familiar with. One of the most common errors in deep
learning, we've already alluded to this as well, is shape errors. So let's jump back to this in a
minute. I just want to write up here. So there are two rules that performing or two main rules
that performing matrix multiplication needs to satisfy. Otherwise, we're going to get an error.
So number one is the inner dimensions must match. Let's see what this means.
So if we want to have two tensors of shape, three by two, and then we're going to use the at symbol.
Now, we might be asking why the at symbol. Well, the at symbol is another, is a like an operator
symbol for matrix multiplication. So I just want to give you an example. If we go tensor at
at stands for matrix multiplication, we get tensor 14, which is exactly the same as what we got there.
Should you use at or should you use mat mall? I would personally recommend to use mat mall.
It's a little bit clearer at sometimes can get confusing because it's not as common as seeing
something like mat mall. So we'll get rid of that, but I'm just using it up here for brevity.
And then we're going to go three, two. Now, this won't work. We'll see why in a second.
But if we go two, three, at, and then we have three, two, this will work. Or, and then if we go
the reverse, say threes on the outside, twos here. And then we have twos on the inside and threes
on the outside, this will work. Now, why is this? Well, this is the rule number one. The inner
dimensions must match. So the inner dimensions are what I mean by this is let's create torch
round or create of size 32. And then we'll get its shape. So we have, so if we created a tensor
like this, three, two, and then if we created another tensor, well, let me just show you straight
up torch dot mat mall torch dot ran to watch this won't work. We'll get an error. There we go. So
this is one of the most common errors that you're going to face in deep learning is that matrix
one and matrix two shapes cannot be multiplied because it doesn't satisfy rule number one.
The inner dimensions must match. And so what I mean by inner dimensions is this dimension multiplied
```

</details>

---

<h3 id='section-27'>Section 27: Main Topics</h3>

**Key Topics:**

- We'll go on with one of those common errors in deep learning shape errors

- Just watch what happens and we're going to replicate something like this in PyTorch code in the next video

- So this is what I've been alluding to as one of the most common errors in deep learning, and that is shape errors

<details>

<summary>📄 Click to view detailed content</summary>

```text
by this dimension. So say we were trying to multiply three, two by three, two, these are the inner
dimensions. Now this will work because why the inner dimensions match. Two, three by three, two,
two, three by three, two. Now notice how the inner dimensions, inner, inner match. Let's see what
comes out here. Look at that. And now this is where rule two comes into play. Two. The resulting
matrix has the shape of the outer dimensions. So we've just seen this one two, three at three, two,
which is at remember is matrix multiply. So we have a matrix of shape, two, three,
matrix multiply a matrix of three, two, the inner dimensions match. So it works. The resulting shape
is what? Two, two. Just as we've seen here, we've got a shape of two, two. Now what if we did
the reverse? What if we did this one that also will work? Three on the outside. What do you think
is going to happen here? In fact, I encourage you to pause the video and give it a go. So this
is going to result in a three three matrix. But don't take my word for it. Let's have a look. Three,
put two on the inside and we'll put two on the inside here and then three on the outside. What
does it give us? Oh, look at that. A three three. One, two, three. One, two, three. Now what if we
were to change this? Two and two. This can be almost any number you want. Let's change them both
to 10. What's going to happen? Will this work? What's the resulting shape going to be? So the
inner dimensions match? What's rule number two? The resulting matrix has the shape of the outer
dimension. So what do you think is going to be the shape of this resulting matrix multiplication?
Well, let's have a look. It's still three three. Wow. Now what if we go 10? 10 on the outside
and 10 and 10 on the inside? What do we get? Well, we get, I'm not going to count all of those,
but if we just go shape, we get 10 by 10. Because these are the two main rules of matrix multiplication
is if you're running into an error that the matrix multiplication can't work. So let's say this was
10 and this was seven. Watch what's going to happen? We can't multiply them because the inner
dimensions do not match. We don't have 10 and 10. We have 10 and seven. But then when we change
this so that they match, we get 10 and 10. Beautiful. So now let's create a little bit more of a
specific example. We'll create two tenses. We'll come down. Actually, to prevent this video from
being too long, I've got an error in the word error. That's funny. We'll go on with one of those
common errors in deep learning shape errors. We've just seen it, but I'm going to get a little bit
more specific with that shape error in the next video. Before we do that, have a look at matrix
multiplication. There's a website, my other favorite website. I told you I've got two. This is my
other one. Matrix multiplication dot XYZ. This is your challenge before the next video. Put in
some random numbers here, whatever you want, two, 10, five, six, seven, eight, whatever you want. Change
these around a bit, three, four. Well, that's a five, not a four. And then multiply and just watch
what happens. That's all I'd like you to do. Just watch what happens and we're going to replicate
something like this in PyTorch code in the next video. I'll see you there.
Welcome back. In the last video, we discussed a little bit more about matrix multiplication,
but we're not done there. We looked at two of the main rules of matrix multiplication,
and we saw a few errors of what happens if those rules aren't satisfied, particularly if the
inner dimensions don't match. So this is what I've been alluding to as one of the most common
errors in deep learning, and that is shape errors. Because neural networks are comprised of lots of
matrix multiplication operations, if you have some sort of tensor shape error somewhere
in your neural network, chances are you're going to get a shape error. So now let's investigate
how we can deal with those. So let's create some tenses, shapes for matrix multiplication.
And I also showed you the website, sorry, matrix multiplication dot xyz. I hope you had a go at
typing in some numbers here and visualizing what happens, because we're going to reproduce
something very similar to what happens here, but with PyTorch code. Shapes for matrix multiplication,
we have tensor a, let's create this as torch dot tensor. We're going to create a tensor with
just the elements one, two, all the way up to, let's just go to six, hey, that'll be enough. Six,
wonderful. And then tensor b can be equal to a torch tensor
of where we're going to go for this one. Let's go seven, 10, this will be a little bit confusing
this one, but then we'll go eight, 11, and this will go up to 12, nine, 12. So it's the same
sort of sequence as what's going on here, but they've been swapped around. So we've got the
vertical axis here, instead of one, two, three, four, this is just seven, eight, nine, 10, 11, 12.
But let's now try and perform a matrix multiplication. How do we do that?
Torch dot mat mall for matrix multiplication. PS torch also has torch dot mm, which stands
for matrix multiplication, which is a short version. So I'll just write down here so that you know
```

</details>

---

<h3 id='section-28'>Section 28: Main Topics</h3>

**Key Topics:**

- This is literally how common matrix multiplications are in PyTorch is that they've made torch dot mm as an alias for mat mall

- But right now we've done it with pytorch code, which might be a little confusing

- But the reason why we're spending so much time on this is because as you'll see, as you get more and more into neural networks and deep learning, the matrix multiplication operation is one of the most or if not the most common

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_27___

___preserve_521___

---

___preserve_684___section 29: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، البته ، شما می توانید این کار را دوباره تنظیم کنید که ممکن است به جای تنش B ، تانسور A را تغییر دهید ، با آن یک نمایشنامه داشته باشید

- و سپس اگر ما روی ضرب کلیک کنیم ، این همان چیزی است که در پشت صحنه با کد Pytorch ما از Matmore اتفاق می افتد

- بنابراین بیایید نگاهی به چند روش pytorch که در حال ساخت هستند برای انجام همه این کارها بیندازیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_28___

___preserve_521___

---

<h3 id='section-30'>Section 30: Main Topics</h3>

**Key Topics:**

- So so far, we've seen two of the major errors in PyTorch is data type and shape issues

- And we also ran into one of the most common issues in pie torch and deep learning and neural networks in general

<details>

<summary>📄 Click to view detailed content</summary>

___preserve_29___

___preserve_521___

---

___preserve_696___Section 31: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین تغییر شکل مجدد قبل از یکی از رایج ترین خطاها در یادگیری ماشین و یادگیری عمیق ، عدم تطابق شکل با ماتریس است زیرا آنها باید قوانین خاصی را برآورده کنند

- از آنجا که دوباره ، یکی از شماره های شماره یک در یادگیری ماشین و یادگیری عمیق ، مسائل مربوط به شکل تانسور است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_30___

___preserve_521___

---

___preserve_702___section 32: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_31___

___preserve_521___

---

___preserve_708___Section 33: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- منظورم این است که ، من هنوز هم این کار را انجام می دهم ، حتی اگر هزاران خط کد یادگیری ماشین را نوشتم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_32___

___preserve_521___

---

___preserve_714___section 34: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- حالا این چیز دیگری است که باید در پیرتر هر وقت که می گوید کم نور باشد ، این ابعاد است که در این یک بعد صفر است ، بعد اول

- به یاد داشته باشید ، بسیاری از موارد ، و من می خواهم به سبک استرالیایی رنگ را هجی کنم ، بسیاری از یادگیری های عمیق تبدیل داده های شما به بازنمایی های عددی است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_33___

___preserve_521___

---

___preserve_720___Section 35: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- آنها به ما کمک می کنند تا مشکلات شکل و بعد را با تانسور ما برطرف کنیم ، که یکی از رایج ترین موضوعات در شبکه های یادگیری عمیق و عصبی است

- و نمای در Pytorch حافظه را با آن تانسور اصلی به اشتراک می گذارد

- بنابراین اگر تا به حال نمایه سازی ، نمایه سازی را انجام داده اید ، با Pytorch شبیه به فهرست بندی با Numpy است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_34___

___preserve_521___

---

___preserve_726___Section 36: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین اکنون بیایید به قسمت بعدی برویم ، که Tensors Pytorch و Numpy است

- Pytorch هنگام نصب Pytorch در واقع به Numpy احتیاج دارد

- و به همین دلیل ، پیوتورچ قابلیت تعامل با آن را دارد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_35___

___preserve_521___

---

___preserve_732___Section 37: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- داده ها ممکن است توسط Numpy نشان داده شوند زیرا در Numpy شروع شده است ، اما می گویند شما می خواهید برخی از یادگیری های عمیق را در مورد آن انجام دهید و می خواهید از قابلیت های یادگیری عمیق Pytorch استفاده کنید ، خوب ، ممکن است بخواهید داده های خود را از Numpy به یک تنش Pytorch تغییر دهید

- و Pytorch روشی برای انجام این کار دارد ، که از Numpy مشعل است ، که در یک آرایه ND ، که نوع اصلی داده Numpy است ، می گیرد و آن را به یک تنش مشعل تغییر می دهد

- و سپس اگر می خواهید از Pytorch Tensor به Numpy بروید زیرا می خواهید از نوعی روش Numpy استفاده کنید ، خوب ، روش انجام این کار Tonch Dot Tensor است و می توانید Dot Numpy را روی آن قرار دهید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_36___

___preserve_521___

---

___preserve_738___Section 38: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اگر می خواهید این موضوع را جستجو کنید ، من شما را تشویق می کنم که تکرارپذیری Pytorch را جستجو کنید و ببینید چه چیزی می توانید پیدا کنید

- اما اگر می خواهید این نوت بوک را با یک دوست به اشتراک بگذارید ، بنابراین بگویید که به اشتراک رفته اید و روی پیوند اشتراک کلیک کردید و آن را برای کسی ارسال کردید و دوست دارید ، سلام ، این آزمایش یادگیری ماشین را که انجام دادم امتحان کنید

- و پیوتورچ مفهوم یک دانه تصادفی است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_37___

___preserve_521___

---

___preserve_744___Section 39: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما به Pytorch می گوییم که تصادفی ما را با 42 دانه دستی مشعل می کنیم

- بنابراین این برنامه درسی اضافی شما برای این کار است ، حتی اگر نمی فهمید که در بسیاری از کد ها در اینجا چه می گذرد ، فقط از تکرارپذیری آگاه باشید ، زیرا این یک موضوع مهم در یادگیری ماشین و یادگیری عمیق است

- شما می توانید این دوره را در ردیف رایگان نیز انجام دهید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_38___

___preserve_521___

---

___preserve_750___Section 40: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین یکی از پست های مورد علاقه من برای دریافت GPU ، بله ، بهترین GPU برای یادگیری عمیق در سال 2020 یا چیزی شبیه به این است

- یادگیری عمیق

- این ، بله ، که GPU ها برای یادگیری عمیق به دست می آیند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_39___

___preserve_521___

---

___preserve_756___Section 41: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اگر در دسترس نباشد ، اگر به GPU دسترسی نداریم که Pytorch بتواند از آن استفاده کند ، فقط به CPU پیش فرض کنید

- اما وقتی آزمایش های Pytorch و آزمایش های یادگیری ماشین را ارتقا می دهید ، ممکن است به بیش از یک GPU دسترسی داشته باشید

- اما نکته نهایی قبل از اتمام این ویدیو این است که اگر ما به کد آگنوستیک دستگاه Pytorch ، Cuda معناشناسی برویم ، بخش کوچکی در اینجا وجود دارد که به نام بهترین روشها وجود دارد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_40___

___preserve_521___

---

___preserve_762___Section 42: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- سه خطای برتر را در یادگیری عمیق یا Pytorch به یاد داشته باشید

- و با Pytorch ، شماره سه مشکلات دستگاه است

- این یک چیز زیبا در مورد Pytorch پیام های خطای بسیار مفیدی است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_41___

___preserve_521___

---

___preserve_768___Section 43: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- همانطور که در طول دوره می رویم ، این تمرینات با عمق کمی بیشتر می شوند زیرا ما بیشتر آموخته ایم

- این خانه برای همه مواد دوره است

- بنابراین تمرینات اصول Pytorch

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_42___

___preserve_521___

---

___preserve_774___Section 44: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و اکنون می خواهم این گردش کار 01 Pytorch را عنوان کنم

- از آنجا که در منابع دوره ، ما نوت بوک اصلی را در اینجا داریم ، این همان چیزی است که این نوت بوک ویدیویی مستقر خواهد شد

- و پس از آن ، شما نسخه کتاب نوت بوک را نیز دریافت کرده اید ، که فقط یک نسخه فرمت متفاوت از همین نوت بوک دقیقاً همان است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_43___

___preserve_521___

---

<h3 id='section-45'>Section 45: Main Topics</h3>

**Key Topics:**

- Now, it might not be exactly like this, but that's the beauty of pytorch is that you can combine these in almost any different way to build any kind of neural network you can imagine

- And let's check our pytorch version

- Pytorch version torch dot version

<details>

<summary>📄 Click to view detailed content</summary>

```text
Now, it might not be exactly like this, but that's the beauty of pytorch is that you can
combine these in almost any different way to build any kind of neural network you can imagine.
And so let's keep going. That's torch nn. We're going to get hands on with it,
rather than just talk about it. And we're going to need map plot lib because what's our other
motto? Our data explorers motto is visualize, visualize, visualize. And let's check our pytorch
version. Pytorch version torch dot version. So this is just to show you you'll need
at least this version. So 1.10 plus CUDA 111. That means that we've got CU stands for CUDA.
That means we've got access to CUDA. We don't have a GPU on this runtime yet,
because we haven't gone to GPU. We might do that later.
So if you have a version that's lower than this, say 1.8, 0.0, you'll want pytorch 1.10 at least.
If you have a version higher than this, your code should still work. But that's about enough
for this video. We've got our workflow ready to set up our notebook, our video notebook.
We've got the resources. We've got what we're covering. We've got our dependencies.
Let's in the next one get started on one data, preparing and loading.
I'll see you in the next video.
Let's now get on to the first step of our pytorch workflow. And that is data, preparing and loading.
Now, I want to stress data can be almost anything in machine learning.
I mean, you could have an Excel spreadsheet, which is rows and columns,
nice and formatted data. You could have images of any kind. You could have videos. I mean,
YouTube has lots of data. You could have audio like songs or podcasts. You could have even DNA
these days. Patents and DNA are starting to get discovered by machine learning. And then, of course,
you could have text like what we're writing here. And so what we're going to be focusing on
throughout this entire course is the fact that machine learning is a game of two parts.
So one, get data into a numerical representation to build a model to learn patterns in that
numerical representation. Of course, there's more around it. Yes, yes, yes. I understand you can
get as complex as you like, but these are the main two concepts. And machine learning, when I say
machine learning, saying goes for deep learning, you need some kind of, oh, number a call. Number
a call. I like that word, number a call representation. Then you want to build a model to learn patterns
in that numerical representation. And if you want, I've got a nice pretty picture that describes that
machine learning a game of two parts. Let's refer to our data. Remember, data can be almost
anything. These are our inputs. So the first step that we want to do is create some form
of numerical encoding in the form of tenses to represent these inputs, how this looks will be
dependent on the data, depending on the numerical encoding you choose to use. Then we're going to
build some sort of neural network to learn a representation, which is also referred to as
patterns features or weights within that numerical encoding. It's going to output that
representation. And then we want to do something without representation, such as in the case of
this, we're doing image recognition, image classification, is it a photo of Raman or spaghetti?
Is this tweet spam or not spam? Is this audio file saying what it says here? I'm not going to say
this because my audio assistant that's also named to this word here is close by and I don't want it
to go off. So this is our game of two parts. One here is convert our data into a numerical
representation. And two here is build a model or use a pre trained model to find patterns in
that numerical representation. And so we've got a little stationary picture here, turn data into
numbers, part two, build a model to learn patterns in numbers. So with that being said,
now let's create some data to showcase this. So to showcase this, let's create some known
data using the linear regression formula. Now, if you're not sure what linear regression is,
or the formula is, let's have a look linear regression formula. This is how I'd find it.
Okay, we have some fancy Greek letters here. But essentially, we have y equals a function of x
and b plus epsilon. Okay. Well, there we go. A linear regression line has the equation in the
form of y equals a plus bx. Oh, I like this one better. This is nice and simple. We're going to
start from as simple as possible and work up from there. So y equals a plus bx, where x is the
explanatory variable, and y is the dependent variable. The slope of the line is b. And the
slope is also known as the gradient. And a is the intercept. Okay, the value of when y
when x equals zero. Now, this is just text on a page. This is formula on a page. You know how I
like to learn things? Let's code it out. So let's write it here. We'll use a linear regression formula
to make a straight line with known parameters. I'm going to write this down because parameter
is a common word that you're going to hear in machine learning as well. So a parameter is
something that a model learns. So for our data set, if machine learning is a game of two parts,
we're going to start with this. Number one is going to be done for us, because we're going to
start with a known representation, a known data set. And then we want our model to learn that
```

___preserve_521___

---

___preserve_786___Section 46: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- خوب ، به این دلیل است که به طور معمول x در یادگیری ماشین می یابید یک ماتریس یا تنش است

- البته ، ما می دانیم که رابطه بین x و y چیست زیرا ما این فرمول را در اینجا کدگذاری کرده ایم

- این کل فرضیه یادگیری ماشین است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_45___

___preserve_521___

---

___preserve_792___Section 47: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین بگویید که سعی می کنید چیزی را در دانشگاه یا از طریق این دوره یاد بگیرید ، ممکن است تمام مطالب را داشته باشید ، که این مجموعه آموزش شماست

- اکنون ، بیایید فقط ببینیم که آیا شما مواد دوره را به خوبی یاد می گیرید

- و این برای دیدن اینکه آیا شما کل مواد درسی را پشت سر گذاشته اید ، و مواردی را یاد گرفته اید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_46___

___preserve_521___

---

___preserve_798___Section 48: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- یکی از بزرگترین ، بزرگترین ، بزرگترین موانع شما در یادگیری ماشین ، ایجاد مجموعه های آموزش و آزمایش مناسب است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_47___

___preserve_521___

---

___preserve_804___Section 49: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین تمام ایده در مورد آنچه ما با مدل یادگیری ماشین خود انجام خواهیم داد این است که ما واقعاً نیازی به ساخت یک مدل یادگیری ماشین برای این کار نداریم

- ما می توانیم کارهای دیگری انجام دهیم ، اما یادگیری ماشین سرگرم کننده است

- اما اکنون بیایید اولین مدل Pytorch خود را بسازیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_48___

___preserve_521___

---

___preserve_810___Section 50: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- به ما کمک می کند تا مدل های Pytorch خود را بسازیم

- و البته ، چگونه می توانید در مورد آن اطلاعات بیشتری کسب کنید

- ماژول ، پیکتورچ

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_49___

</details>

---

<h3 id='section-51'>Section 51: Main Topics</h3>

**Key Topics:**

- learning correctly, will take our weight, which is going to be a random value, and our bias, which is going to be a random value

- So that's the premise of machine learning

- And so what this is going to do is when we run computations using this model here, pytorch is going to keep track of the gradients of our weights parameter and our bias parameter

<details>

<summary>📄 Click to view detailed content</summary>

```text
learning correctly, will take our weight, which is going to be a random value, and our bias,
which is going to be a random value. And it will run it through this forward calculation,
which is the same formula that we use to create our data. And it will adjust the weight and bias
to represent as close as possible, if not perfect, the known parameters. So that's the premise of
machine learning. And how does it do this? Through an algorithm called gradient descent. So I'm just
going to write this down because we've talked a lot about this, but I'd like to just tie it together
here. So what our model does, so start with random values, weight and bias, look at training data,
and adjust the random values to better represent the, or get closer to the ideal values. So the
weight and bias values we use to create the data. So that's what it's going to do. It's going to
start with random values, and then continually look at our training data to see if it can adjust
those random values to be what would represent this straight line here. Now, how does it do so?
How does it do so? Through two main algorithms. So one is gradient descent, and two is back
propagation. So I'm going to leave it here for the time being, but we're going to continue talking
about this gradient descent is why we have requires grad equals true. And so what this is going to
do is when we run computations using this model here, pytorch is going to keep track of the gradients
of our weights parameter and our bias parameter. And then it's going to update them through a
combination of gradient descent and back propagation. Now, I'm going to leave this as extracurricular
for you to look through and gradient descent and back propagation. I'm going to add some
resources here. There will also be plenty of resources in the pytorch workflow fundamentals
book chapter on how these algorithms work behind the scenes. We're going to be focused on the code,
the pytorch code, to trigger these algorithms behind the scenes. So pytorch, lucky for us,
has implemented gradient descent and back propagation for us. So we're writing the higher level code
here to trigger these two algorithms. So in the next video, we're going to step through this a
little bit more, and then further discuss some of the most useful and required modules of pytorch,
particularly an N and a couple of others. So let's leave it there, and I'll see you in the next video.
Welcome back. In the last video, we covered a whole bunch in creating our first pytorch model
that inherits from nn.module. We talked about object oriented programming and how a lot of
pytorch uses object oriented programming. I can't say that. I might just say OOP for now.
What I've done since last video, though, is I've added two resources here for gradient descent
and back propagation. These are two of my favorite videos on YouTube by the channel three blue
one brown. So this is on gradient descent. I would highly recommend watching this entire series,
by the way. So that's your extra curriculum for this video, in particular, and for this course overall
is to go through these two videos. Even if you're not sure entirely what's happening,
you will gain an intuition for the code that we're going to be writing with pytorch.
So just keep that in mind as we go forward, a lot of what pytorch is doing behind the scenes for us
is taking care of these two algorithms for us. And we also created two parameters here in our model
where we've instantiated them as random values. So one parameter for each of the ones that we use,
the weight and bias for our data set. And now I want you to keep in mind that we're working
with a simple data set here. So we've created our known parameters. But in a data set that you
haven't created by yourself, you've maybe gathered that from the internet, such as images,
you won't be necessarily defining these parameters. Instead, another module from nn will define the
parameters for you. And we'll work out what those parameters should end up being. But since we're
working with a simple data set, we can define our two parameters that we're trying to estimate.
This is a key point here is that our model is going to start with random values. That's the
annotation I've added here. Start with a random weight value using torch random. And then we've
told it that it can update via gradient descent. So pytorch is going to track the gradients of
this parameter for us. And then we've told it that the D type we want is float 32. We don't
necessarily need these two set explicitly, because a lot of the time the default in pytorch is to
set these two requires grad equals true and d type equals torch dot float. Does that for us
behind the scenes? But just to keep things as fundamental and as straightforward as possible,
we've set all of this explicitly. So let's jump into the keynote. I'd just like to explain
what's going on one more time in a visual sense. So here's the exact code that we've
just written. I've just copied it from here. And I've just made it a little bit more colorful.
But here's what's going on. So when you build a model in pytorch, it subclasses the nn.modgable
class. This contains all the building blocks for neural networks. So our class of model, subclasses
nn.modgable. Now, inside the constructor, we initialize the model parameters. Now, as we'll see,
later on with bigger models, we won't necessarily always explicitly create the weights and biases.
```

</details>

---

<h3 id='section-52'>Section 52: Main Topics</h3>

** مباحث کلیدی: **

- بنابراین این ، به نوبه خود ، به این معنی است که Pytorch در پشت صحنه تمام شیب های این پارامترها را در اینجا برای استفاده با مشعل ردیابی می کند

- ماژول درجه خودکار Pytorch همان چیزی است که نزول شیب را اجرا می کند

- اکنون ، بسیاری از این اتفاقات در پشت صحنه اتفاق می افتد که وقتی کد آموزش Pytorch خود را می نویسیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_51___

___preserve_521___

---

___preserve_828___Section 53: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- Pytorch ماژول های اصلی ساختمان شبکه عصبی است

- و اگر بیشتر دوست دارید ، یکی از منابع مورد علاقه من برگه تقلب Pytorch است

- همانطور که گفتم ، این دوره جایگزینی برای مستندات نیست

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_52___

___preserve_521___

---

___preserve_834___Section 54: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- مدل صفر ، زیرا این مدل Zeroth خواهد بود ، اولین مدلی که ما در کل این دوره ایجاد کرده ایم ، چقدر مدل رگرسیون خطی شگفت انگیز است ، این همان چیزی است که کلاس ما نامیده می شود

- بنابراین این جوهر کاری است که مدل های یادگیری ماشین ما و مدل های یادگیری عمیق انجام می دهند

- و بنابراین ، ما نمی خواهیم همه آنها را با دست انجام دهیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_53___

___preserve_521___

---

___preserve_840___Section 55: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- در آخرین ویدئو ، ما داخلی های اولین مدل Pytorch خود را بررسی کردیم

- و ما همچنین بحث کردیم که کل فرضیه یادگیری عمیق این است که با اعداد تصادفی شروع شود و به آرامی آنهایی را به سمت اعداد ایده آل تر ، تعداد کمی تصادفی کمتر بر اساس داده ها پیشرفت کنیم.

- از آنجا که دوباره به یاد داشته باشید ، یک فرض دیگر از یک مدل یادگیری ماشین ، گرفتن برخی از ویژگی ها به عنوان ورودی و ایجاد برخی از پیش بینی ها به نوعی از برچسب ها است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_54___

___preserve_521___

---

___preserve_846___Section 56: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این چیزی به نام یادگیری انتقال است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_55___

___preserve_521___

---

___preserve_852___Section 57: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما در حال آموزش اولین مدل یادگیری ماشین خود هستیم

- و بنابراین اگر ما به توابع از دست دادن Pytorch برویم ، می خواهیم ببینیم که Pytorch دارای عملکردهای ضرر کمی است که در آن ساخته شده است

- ما در حال یادگیری چیزی سرگرم کننده هستیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_56___

___preserve_521___

---

___preserve_858___Section 58: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- منظورم این است که کل دوره سرگرم کننده است

- اما این واقعاً هیجان انگیز است زیرا آموزش اولین مدل یادگیری ماشین شما کمی شبیه جادو به نظر می رسد ، اما وقتی خودتان کد می نویسید که در پشت صحنه اتفاق می افتد حتی سرگرم کننده تر است

- این ایده کل یک حلقه آموزش در Pytorch یا یک حلقه بهینه سازی در Pytorch است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_57___

___preserve_521___

---

___preserve_864___Section 59: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- هدف ما برای آموزش یک مدل یادگیری ماشین دو خواهد بود

- غالباً انتخاب یک عملکرد ضرر و بهینه ساز و پیرتر به عنوان بخشی از همان بسته بندی می شوند زیرا آنها با هم کار می کنند

- بنابراین ، دوباره ، پیکتورچ مشعل دارد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_58___

___preserve_521___

---

___preserve_870___Section 60: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین نرخ یادگیری با احتمالاً مهمترین پارامتر Hyper Learning برابر است

- من در آنجا نیازی به یادگیری ندارم ،

- و یک پارامتر Hyper ارزشی است که ما به عنوان یک دانشمند داده یا یک مهندس یادگیری ماشین خودمان را تنظیم می کنیم ، می توانید تنظیم کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_59___

___preserve_521___

---

___preserve_876___Section 61: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بسیاری از افراد ، یکی از اصلی ترین مواردی که از یادگیری ماشین خواسته می شود این است که اگر ریاضی را انجام ندادم یادگیری ماشین را چگونه یاد بگیرم

- خوب ، نکته زیبا در مورد Pytorch این است که بسیاری از ریاضیات انتشار پشت را پیاده سازی می کند

- بنابراین این دو الگوریتم اکثر یادگیری ما را هدایت می کنند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_60___

___preserve_521___

---

___preserve_882___Section 62: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و من می دانم که شما می توانید استدلال کنید که ، سلام ، پارامترهای یادگیری ماشین ما از مدل صفر یا پارامترهای مدل ما ، مدل صفر در واقع پارامترهایی نیستند ، زیرا ما آنها را تنظیم کرده ایم

- بنابراین مدل های Pytorch چند حالت مختلف دارند

- بنابراین حالت قطار در یک مدل Pytorch چه کاری انجام می دهد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_61___

___preserve_521___

---

___preserve_888___Section 63: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و در مورد ما ، ما می خواهیم ضرر را برابر با عملکرد ضرر خود تنظیم کنیم ، که یک ضرر در Pytorch است ، اما این MAE است

- اما در مورد صادق ماندن به مستندات ، بیایید ابتدا ورودی ها را انجام دهیم و سپس برای بقیه دوره هدف قرار دهیم

- و اکنون دلیل جمع آوری آن ، این در اسناد Pytorch بسیار عمیق است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_62___

___preserve_521___

---

___preserve_894___Section 64: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و اگر نسخه ویدیویی آن را می خواهید ، خوب ، شما فقط باید آهنگ حلقه بهینه سازی Pytorch غیررسمی را جستجو کنید

- در چند فیلم اخیر ، ما در مورد مراحل در یک حلقه آموزش در Pytorch بحث کرده ایم

- در اصل ، این مجموعه کل تنظیمات را در پشت صحنه در پارامترهای مدل ما تنظیم می کند تا آنها بتوانند شیب ها را ردیابی کنند و یک دسته کامل از یادگیری را در پشت صحنه با این توابع در اینجا انجام دهند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_63___

___preserve_521___

---

___preserve_900___Section 65: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ما آن را در یادگیری ماشین می خواهیم

- چه چیزی در یادگیری ماشین به ما می دهد ، یک شیب مشتق یک عملکرد است که بیش از یک متغیر ورودی دارد

- و سپس ما یک مرحله یادگیری برداشته می شویم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_64___

___preserve_521___

---

___preserve_906___Section 66: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این زیبایی پیتورچ است

- سپس در بهینه ساز ، هنگامی که به آن گفتیم که چه پارامترهایی برای بهینه سازی ، ما نرخ یادگیری را داریم

- بنابراین میزان یادگیری پارامتر Hyper دیگری است که تعریف می کند که بهینه ساز چقدر بزرگ یا کوچک پارامترها را با هر مرحله تغییر می دهد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_65___

___preserve_521___

---

___preserve_912___section 67: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- به یاد داشته باشید ، این دوره بر نوشتن کد Pytorch متمرکز است

- اما اگر می خواهید به آنچه که Pytorch ریاضی در پشت صحنه ایجاد می کند ، شیرجه بزنید ، من این دو فیلم را به شدت توصیه می کنم

- اما با تجربه با گذشت زمان ، شما با مشکلات یادگیری ماشین کار می کنید ، کد زیادی می نویسید ، ایده ای راجع به آنچه کار می کند و چه چیزی با مجموعه مشکل خاص شما نیست ، می گیرید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_66___

___preserve_521___

---

___preserve_918___Section 68: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما اندازه گیری شیب Pytorch این کار را در پشت صحنه برای ما انجام داده است

- متشکرم Pytorch

- ما اولین مدل یادگیری ماشین خود را آموزش می دهیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_67___

___preserve_521___

---

___preserve_924___Section 69: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین ما بحث کردیم که اگر پارامترهای موجود در مدل ما به درجه یک برابر نیاز دارند ، که به طور پیش فرض برای بسیاری از پارامترهای مختلف در Pytorch است ، پیکتورچ در پشت صحنه خواهد بود که شیب های مدل ما را ردیابی می کند و از آنها در مرحله از دست رفته به عقب و بهینه ساز برای انتشار عقب و شیب نزولی استفاده می کند

- با این حال ، ما فقط در طول آموزش به آن دو نسخه پشتی و نزول شیب نیاز داریم زیرا این زمانی است که مدل ما در حال یادگیری است

- بنابراین ما هنگام آزمایش نیازی به یادگیری نداریم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_68___

___preserve_521___

---

___preserve_930___Section 70: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما زیبایی Pytorch این است که شما می توانید از بیانیه های اساسی چاپ پایتون استفاده کنید تا ببینید چه چیزی با مدل شما اتفاق می افتد

- این است ، من نمی توانم به اندازه کافی استرس داشته باشم ، مانند آنچه ما در اینجا انجام می دهیم در کل بقیه دوره برای آموزش بیشتر و بیشتر مدل ها بسیار شبیه به هم خواهد بود

- بنابراین این مرحله که ما در اینجا برای آموزش مدل خود انجام داده ایم و ارزیابی آن به طور جدی مانند مراحل اساسی یادگیری عمیق با Pytorch آموزش و ارزیابی یک مدل است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_69___

___preserve_521___

---

___preserve_936___Section 71: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته ، ما فقط می توانیم یک مدل ایجاد کنیم و پارامترها را به صورت دستی تنظیم کنیم

- ما فقط برخی از کد یادگیری ماشین را نوشتیم تا این کار را برای ما با قدرت انتشار پشت و نزول شیب انجام دهیم

- ما ممکن است از نرخ یادگیری متفاوتی استفاده کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_70___

___preserve_521___

---

___preserve_942___Section 72: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین کاری که ما در اینجا انجام می دهیم این است که مقادیر ضرر ما هنوز در Pytorch است ، و آنها نمی توانند به این دلیل باشند که Mapplotlib با Numpy کار می کند

- یکی از زیباترین طرفهای یادگیری ماشین ، منحنی از دست دادن در حال کاهش است

- و البته ، پایین تر بهتر است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_71___

___preserve_521___

---

___preserve_948___Section 73: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اوه ، به هر حال ، اگر می خواهید به همه برنامه های درسی اضافی پیوند دهید ، فقط به نسخه کتاب دوره بروید

- بنابراین آنچه ممکن است در این فیلم پوشش دهیم ، صرفه جویی در یک مدل در Pytorch است

- بنابراین اکنون بیایید ببینیم که چگونه می توانیم مدل های خود را در Pytorch ذخیره کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_72___

___preserve_521___

---

___preserve_954___Section 74: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین روش توصیه شده برای صرفه جویی و بارگیری یک مدل Pytorch با صرفه جویی در عرشه حالت خود است

- بنابراین Pytorch کد را ذخیره و بارگیری کنید

- بنابراین اگر می خواهیم مدل Pytorch خود را ذخیره کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_73___

___preserve_521___

---

___preserve_960___Section 75: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، البته ، ما یک مدل را ذخیره کرده ایم

- بنابراین ما به اینجا برمی گردیم و ما یک مدل Pytorch را بارگیری خواهیم کرد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_74___

___preserve_521___

---

___preserve_966___Section 76: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین این در حال صرفه جویی و بارگیری یک مدل در Pytorch است

- در طی چند فیلم گذشته ، ما یک دسته کامل از زمین را در یک گردش کار Pytorch پوشانده ایم ، با شروع داده ها ، سپس ساختن یک مدل

- سپس یاد گرفتیم که چگونه یک مدل را در Pytorch ذخیره و بارگذاری کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_75___

___preserve_521___

---

___preserve_972___Section 77: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما در طول دوره می خواهیم آن ها را بپوشانیم

- بنابراین بیایید با وارد کردن Pytorch شروع کنیم

- و ما قصد داریم نسخه Pytorch را بررسی کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_76___

___preserve_521___

---

___preserve_978___Section 78: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- چیزهای زیادی در یادگیری ماشین وجود دارد که کاملاً انعطاف پذیر هستند

- این تنش است زیرا ما از Pytorch برای ایجاد آن استفاده می کنیم

- بنابراین ساختن یک مدل خطی Pytorch

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_77___

___preserve_521___

---

___preserve_984___Section 79: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و من ساخت یک مدل خطی Pytorch را در اینجا قرار داده ام

- ماژول به همین دلیل بسیاری از مدل های Pytorch ، زیر کلاس و سپس ماژول

- به یاد داشته باشید ، NN در Pytorch مخفف شبکه عصبی است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_78___

___preserve_521___

---

___preserve_990___Section 80: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این در پشت صحنه چگونگی ایجاد Pytorch لایه های مختلف خود قرار می گیرد

- بنابراین این سبک از آنچه در اینجا اتفاق می افتد این است که چگونه می خواهید اکثر مدل های یادگیری عمیق Pytorch خود را با استفاده از لایه های از قبل موجود از مشعل ایجاد کنید

- بنابراین برای همه لایه های مشترک در یادگیری عمیق ، زیرا این همان چیزی است که شبکه های عصبی هستند ، آنها لایه هایی از تحولات ریاضی مختلف هستند ، پیکتورچ اجرای پیش ساخته زیادی دارد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_79___

___preserve_521___

---

___preserve_996___Section 81: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- خوب ، Pytorch بهینه سازهای زیادی را در TORCH DOT OPT در SGD ارائه می دهد

- LR ، که مخفف نرخ یادگیری است

- به عبارت دیگر ، بهینه ساز ما چقدر بزرگ پارامترهای ما را با هر تکرار تغییر می دهد ، نرخ یادگیری کمتری

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_80___

___preserve_521___

---

___preserve_1002___Section 82: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اوه ، البته

- بله ، البته ، این همان اتفاقی است که افتاده است

- بنابراین به یاد داشته باشید ، یکی از بزرگترین مسائل مربوط به Pytorch جدا از خطاهای شکل این است که شما باید داده های خود یا همه مواردی را که با همان دستگاه محاسبه می کنید داشته باشید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_81___

___preserve_521___

---

___preserve_1008___Section 83: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اوه ، البته

- البته از Matplotlib استفاده می کند ، و Matplotlib با Numpy کار می کند ، نه Pytorch

- بنابراین ، مطمئناً ما در اینجا با خطای دیگری روبرو هستیم ، زیرا ما فقط گفتیم که پیش بینی های ما در دستگاه CUDA است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_82___

___preserve_521___

---

___preserve_1014___Section 84: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و مسیر پرونده ای که می خواهیم از آن استفاده کنیم ، البته مدل ذخیره سازی مدل است که ما در اینجا دیدیم یک مسیر POSIX است

- این یکی برای ما از گردش کاری که قبلاً در اینجا انجام دادیم ، با صرفه جویی در یک مدل در Pytorch ، بارگذاری یک مدل Pytorch

- و اکنون کسی که ما بدست آورده ایم ، مدل اول همان چیزی است که ما به تازگی ذخیره کرده ایم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_83___

___preserve_521___

---

___preserve_1020___Section 85: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- رفتن به شما به شما نشان می دهد که در آن می توانید برخی از تمرینات و تمام برنامه های درسی اضافی را که در طول این بخش 01 گردش کار Pytorch صحبت کرده ام پیدا کنید

- در آخرین ویدئو ، ما با صرفه جویی و بارگیری مدل آموزش دیده خود ، که بسیار هیجان انگیز است ، به پایان رسیدیم ، زیرا بیایید به انتهای بخش گردش کار Pytorch بیاییم

- بنابراین در نسخه کتاب مواد دوره ، که در LearnyPytorch است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_84___

___preserve_521___

---

___preserve_1026___Section 86: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- انجام این کار در دوره GitHub ، که در صفحه بحث و گفتگو قرار خواهد گرفت ، که در اینجا مرتبط است

- اگر این را بارگیری کنیم ، هنوز هیچ چیز در اینجا وجود ندارد ، زیرا وقتی این فیلم ها را ضبط می کنم ، دوره هنوز راه اندازی نشده است ، اما بحث جدید را فشار دهید

- سرانجام ، فراموش نکنید که این دفترچه که ما در حال گذراندن آن هستیم ، مبتنی بر فصل دو از صفر به تسلط است که Pytorch را برای یادگیری عمیق بیاموزد ، که طبقه بندی شبکه عصبی با Pytorch است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_85___

___preserve_521___

---

___preserve_1032___Section 87: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- به عبارت دیگر ، زیرا به یاد داشته باشید ، مدل های یادگیری ماشین ، شبکه های عصبی دوست دارند ورودی های عددی داشته باشند

- خوب ، البته ، ما می خواهیم بخشی از آشپز ، بخشی شیمیدان ، هنرمند جزئی ، علوم جزئی باشیم

- خوب ، بیایید آن را به ورودی ها ، نوعی الگوریتم یادگیری ماشین و سپس خروجی ها تقسیم کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_86___

___preserve_521___

---

___preserve_1038___Section 88: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما می توانیم از Pytorch برای ساختن یک الگوریتم یادگیری ماشین برای این کار استفاده کنیم

- خوب ، در این حالت ، این احتمالات پیش بینی هستند ، که خروجی مدل های یادگیری ماشین هرگز در واقع گسسته نیستند ، به این معنی که قطعاً پیتزا است

- این ایده کامل برای یادگیری ماشین است ، این است که اگر الگوریتم را تنظیم کنید ، اگر داده ها را تنظیم کنید ، می توانید پیش بینی های خود را بهبود بخشید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_87___

___preserve_521___

---

___preserve_1044___Section 89: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، البته ، همانطور که تصور می کنید ، این بسته به مشکلی که با آن کار می کنید ممکن است تغییر کند

- و به هر حال ، اگر ما به نسخه کتاب دوره برویم ، این مسئله از این طریق است.

- بنابراین تمام این متن در LearnypyTorch در دسترس است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_88___

___preserve_521___

---

___preserve_1050___Section 90: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- RELU ، که یک واحد خطی اصلاح شده است ، اما می تواند بسیاری دیگر باشد زیرا البته Pytorch دارای چه چیزی است

- بنابراین برای طبقه بندی باینری ، ممکن است از از دست دادن آنتروپی باینری در Pytorch استفاده کنیم و برای طبقه بندی چند طبقه ، فقط ممکن است از آنتروپی متقاطع به جای آنتروپی متقاطع باینری استفاده کنیم.

- یکی دیگر از گزینه های متداول اتم بهینه ساز و البته مشعل است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_89___

___preserve_521___

---

___preserve_1056___Section 91: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_90___

___preserve_521___

---

___preserve_1062___Section 92: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این یک چیز رایج است که در یادگیری ماشین نیز می شنوید

-بنابراین اگر می خواهید در مورد برخی از مجموعه های داده اطلاعات بیشتری کسب کنید که می توانید در اینجا نگاهی بیندازید و به طور بالقوه با شبکه های عصبی یا سایر اشکال مدل های یادگیری ماشین از Scikit-Learn تمرین کنید ، این Scikit-Learn را بررسی کنید

- من می دانم که این یک دوره Torch است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_91___

___preserve_521___

---

___preserve_1068___Section 93: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و برای این کار ، ما باید مشعل را وارد کنیم ، Pytorch را دریافت کنیم و نسخه مشعل را بررسی خواهیم کرد

- فقط اطمینان حاصل کنید که می توانیم Pytorch را وارد کنیم

- با این حال ، Pytorch ، نوع پیش فرض شناور 32 است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_92___

___preserve_521___

---

___preserve_1074___Section 94: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- با این حال ، از آنجا که ما از Scikit Learn استفاده می کنیم ، تنظیم دانه دستی مشعل فقط بر کد Pytorch تأثیر می گذارد تا کد یادگیری Scikit

- بنابراین در ویدیوی بعدی ، ما اکنون مجموعه های آموزش و آزمایش را دریافت کرده ایم ، ما شروع به حرکت از طریق گردش کار زیبای Pytorch در اینجا کرده ایم

- در واقع ، همه مدل های موجود در زیر کلاس Pytorch و در پایان ماژول

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_93___

___preserve_521___

---

___preserve_1080___Section 95: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین تقریباً همه مدل ها در Pytorch ، Subclass ، و سپس ماژول دریافت کردند زیرا کارهای بسیار خوبی وجود دارد که این کار را برای ما در پشت صحنه انجام می دهد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_94___

___preserve_521___

---

___preserve_1086___Section 96: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- من به اندازه کافی در مورد سخت افزار رایانه نمی دانم که دقیقاً می دانم چرا این مورد است ، اما این فقط یک قانون شست در یادگیری ماشین است

- اما واقعاً ، این همان چیزی است که اکثر مدل های یادگیری ماشین ساختمان در Pytorch به نظر می رسد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_95___

___preserve_521___

---

___preserve_1092___Section 97: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اگر به زمین بازی Tensorflow بروید ، و اکنون Tensorflow یکی دیگر از چارچوب های یادگیری عمیق شبیه به Pytorch است ، فقط به شما امکان می دهد کدی مانند این را بنویسید ، برای ساخت شبکه های عصبی ، متناسب با نوعی از داده ها برای یافتن الگوهای و داده ها ، و سپس از آن مدل های یادگیری ماشین در برنامه های خود استفاده کنید.

- و شاید ما نرخ یادگیری را قرار دهیم ، ما میزان یادگیری را تا 0 مشاهده کرده ایم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_96___

___preserve_521___

---

___preserve_1098___Section 98: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین ، این البته قصد دارد مدل قبلی ما را صفر کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_97___

___preserve_521___

---

___preserve_1104___section 99: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما این انعطاف پذیری Pytorch است

- بنابراین ، البته ، اگر ما نگاهی به Deck State Model Zero Dot ، OH داشته باشیم ، این یک آزمایش خوب خواهد بود

- بنابراین این برای اولین لایه Zeroth است ، این دو در اینجا با نقطه صفر ، و سپس وزن یک نقطه چهار است ، البته اولین لایه شاخص

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_98___

___preserve_521___

---

___preserve_1110___Section 100: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و N Dot Sequential یک روش ساده تر برای ایجاد یک مدل Pytorch است

- و سپس در داخل مدل ما ، Pytorch در پشت صحنه باعث ایجاد تانسور وزن و تعصب برای هر یک از لایه های ما با توجه به اشکال تنظیم شده است

- و بنابراین نکته مفید در این مورد این است که اگر ما با لایه های خود کاملاً مسخره می شدیم ، پیوتور هنوز همان کار را در پشت صحنه انجام می داد ، یک دسته کامل از شماره های تصادفی را برای ما ایجاد می کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_99___

___preserve_521___

---

___preserve_1116___section 101: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- سپس به من اعتماد کنید ، هنگامی که من برای اولین بار استفاده از Pytorch را شروع کردم ، کمی گیج شدم که چرا آنها در اینجا دو نفر دارند ، اما ما به هر حال قصد داریم این موضوع را کشف کنیم

- خوشبختانه Pytorch این کار را برای ما انجام می دهد ، اما Logit نوعی در یادگیری عمیق گیج کننده است

- بنابراین اگر ما به یادگیری عمیق بپردازیم ، این به معنای یک چیز متفاوت است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_100___

___preserve_521___

---

___preserve_1122___Section 102: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین مشعل سیگموئید نقطه سیگموئید و پیوتچ

- بنابراین این دو وقتی که حلقه آموزش خود را می نویسیم ، دوباره به پشت سر هم کار می کنند و ما نرخ یادگیری خود را بر 0 تنظیم خواهیم کرد

- خوب ، بیایید ببینیم که آیا می توانیم چیزی شبیه به آن را فقط با استفاده از Pytorch خالص پیاده سازی کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_101___

___preserve_521___

---

___preserve_1128___Section 103: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین اگر به این پاسخ سرریز پشته برویم ، دیدیم که ماشین یادگیری است ، ورود به سیستم چیست

- این بخش مهیج یادگیری ماشین است

- بنابراین این تعریف ورود به سیستم در یادگیری ماشین و یادگیری عمیق است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_102___

___preserve_521___

---

___preserve_1134___Section 104: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین ما به این فرمت نیاز داریم که البته این نیست

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_103___

___preserve_521___

---

___preserve_1140___Section 105: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- آنها در یک قالب یکسان هستند ، اما مطمئناً آنها همان مقادیر نیستند زیرا این مدل از وزنهای تصادفی برای پیش بینی استفاده می کند

- و چگونه می توانیم آن را بررسی کنیم ، البته ، در حالی که می توانیم دستگاه را تایپ کنیم ، می توانیم آن سلول را اجرا کنیم

- آیا ما pytorch داریم؟

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_104___

___preserve_521___

---

___preserve_1146___Section 106: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این شهادت به ماهیت فیثونی Pytorch نیز هست

- به دلایلی ، شما در برخی از کد های Pytorch که از ضرر BCE استفاده می کند ، گیر می کنید ، نه BCE با از دست دادن ورود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_105___

___preserve_521___

---

___preserve_1152___Section 107: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و من در حال خواندن آهنگ غیر رسمی Pytorch بهینه سازی حلقه در آنجا هستم

- و البته ، ما می خواهیم ورود به سیستم را محاسبه کنیم ، زیرا ورود به سیستم خروجی خام مدل ما بدون اصلاح است

- خوب ، فقط اگر قبلاً انجام داده ایم ، و ما می خواهیم ورود به سیستم از دست دادن FN را انجام دهیم ، زیرا عملکرد از دست دادن ما ، ما از آنچه استفاده می کنیم BCE با از دست دادن Logits استفاده می کنیم ، انتظار داریم که به عنوان ورودی وارد شود ، از کجا می فهمیم که در اسناد ، البته ، ما به اینجا باز می گردیم ، ورود به سیستم ، ما می خواهیم آن را با برچسب های آزمون Y مقایسه کنیم.

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_106___

___preserve_521___

---

___preserve_1158___Section 108: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- به نظر نمی رسد که مدل ما چیزی یاد بگیرد

- بنابراین به بررسی اینکه چرا مدل ما در حال یادگیری نیست بپردازید

- به نظر می رسد که مدل ما چیزی یاد نمی گیرد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_107___

___preserve_521___

---

___preserve_1164___Section 109: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین توابع Helper را از Learn Pytorch Repo بارگیری کنید

- اگر به عقب برگردیم ، این فقط Pytorch Deep Learning repo برای این دوره عملکرد یاور برش است

- بنابراین این کد اساساً می گوید سلام درخواست ها ، اطلاعاتی را که در این لینک وجود دارد در اینجا دریافت کنید ، که البته همه این کد در اینجا است ، که یک اسکریپت پایتون است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_108___

___preserve_521___

---

___preserve_1170___Section 110: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین من می خواهم که شما در مورد این فکر کنید ، حتی اگر کاملاً تازه وارد یادگیری عمیق باشید ، آیا می توانیم

- این شبیه به کاری است که ما با Pytorch انجام داده ایم

- Pytorch در اصل فقط مجموعه ای از اسکریپت های پایتون است که ما برای ساخت شبکه های عصبی از آن استفاده می کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_109___

___preserve_521___

---

___preserve_1176___section 111: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین ما می خواهیم در اینجا به برخی از گزینه ها نگاهی بیندازیم ، لایه های بیشتری اضافه کنیم و برای مدت طولانی تر متناسب باشیم ، شاید تغییر نرخ یادگیری

- Tensorboard ابزاری یا ابزاری از Pytorch است که به شما در نظارت بر آزمایش ها کمک می کند

- بنابراین به یاد داشته باشید که چگونه گفتم چند برابر هشتم در یادگیری عمیق بسیار خوب هستند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_110___

___preserve_521___

---

___preserve_1182___Section 112: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- پارامترهای Hyper همان چیزی است که ما به عنوان مهندسین یادگیری ماشین و دانشمندان داده ، مانند اضافه کردن لایه های بیشتر ، واحدهای پنهان تر ، مناسب برای تعداد طولانی تر دوره ها ، عملکردهای فعال سازی ، میزان یادگیری ، عملکردهای از دست دادن پارامترهای بیش از حد هستند زیرا آنها می توانیم تغییر کنیم که می توانیم تغییر دهیم

- اما به طور کلی ، هنگامی که شما آزمایش های یادگیری ماشین را انجام می دهید ، فقط دوست دارید یک مقدار را در یک زمان تغییر داده و نتایج را پیگیری کنید

- بنابراین به آن ردیابی آزمایش و یادگیری ماشین گفته می شود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_111___

___preserve_521___

---

___preserve_1188___Section 113: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این همان LR است که ما قبل از یادگیری از آن استفاده می کنیم

- اوه ، به طور بالقوه ممکن است نرخ یادگیری ما خیلی بزرگ باشد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_112___

___preserve_521___

---

___preserve_1194___Section 114: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- به یاد داشته باشید که یادگیری ماشین نامهای مختلف زیادی برای همین چیزها دارد

- البته ، این از دست دادن آموزش خواهد بود

- اکنون ، البته ، این دقت آموزش خواهد بود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_113___

___preserve_521___

---

___preserve_1200___Section 115: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این Pytorch Learn است

- اصول گردش کار Pytorch کتاب IO

- از آنجا که در حال حاضر به نظر می رسد که اصلاً چیزی یاد نمی گیرد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_114___

___preserve_521___

---

___preserve_1206___Section 116: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- زیرا همه چیز در مورد داده ها و یادگیری ماشین است

- برای بارگیری آن از دوره GitHub ، و ما پیش بینی های نقشه را از آن وارد کردیم

- بنابراین بعداً ، اگر با یک مجموعه داده یادگیری ماشین بزرگ کار می کنید ، احتمالاً ابتدا با بخش کوچکی از آن مجموعه داده شروع می کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_115___

___preserve_521___

---

___preserve_1212___section 117: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- فقط یک ارزش وجود دارد ، بیایید به خودمان یادآوری کنیم ، این اشکال ورودی و خروجی است ، یکی از اساسی ترین چیزها در یادگیری ماشین و یادگیری عمیق

- و البته لایه دوم ، تعداد ویژگی های موجود در اینجا باید با ویژگی های خارج از لایه قبلی مطابقت داشته باشد

- البته ، ما می توانیم این تعداد را بسیار بزرگ کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_116___

___preserve_521___

---

___preserve_1218___Section 118: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، ما می توانیم در اینجا آزمایشاتی انجام دهیم

- و البته ، ما می توانیم این کار را با ساخت این عملکرد کوتاه کنیم

- بنابراین این بدان معنی است که مدل ما باید چیزی یاد بگیرد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_117___

___preserve_521___

---

___preserve_1224___section 119: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این مزیت میزبان این نمایشگاه آشپزی یادگیری ماشین است

- شما می توانید کمی بهتر شوید ، البته نقاط قرمز را در بالای نقاط سبز بدست آورید

- اکنون ، این یکی از چیزهای زیبا در مورد یادگیری ماشین است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_118___

___preserve_521___

---

___preserve_1230___Section 120: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این شما را در تمام یادگیری ماشین و یادگیری عمیق ، غیرخطی دنبال می کند

- یا از نظر یادگیری ماشین ، یک مقدار نامحدود ، اما واقعاً محدود است

- توسط بی نهایت از نظر یادگیری ماشین ، این یک فنی است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_119___

___preserve_521___

---

___preserve_1236___Section 121: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما Pytorch برخی از متداول ترین لایه هایی را که شما به عنوان لایه های پنهان دارید ، پیاده سازی می کند

- و البته اگر می خواستید ، می توانید به دنبال این باشید که یک عملکرد غیرخطی چیست

- در حالی که شبکه های عصبی و مدل های یادگیری ماشین می توانند با شماره هایی که در صدها بعد قرار دارند کار کنند ، تجسم برای ما غیرممکن است ، اما از آنجا که رایانه ها عاشق اعداد هستند ، این یک تکه کیک برای آنها است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_120___

___preserve_521___

---

___preserve_1242___Section 122: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و سپس نرخ یادگیری را به 0 قرار دهید

- اما نکات اصلی در اینجا نیاز به یادگیری میزان 0 است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_121___

___preserve_521___

---

___preserve_1248___Section 123: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و اگر نرخ یادگیری را تغییر دهیم ، شاید کمی پایین تر باشد ، بیایید ببینیم چه اتفاقی می افتد

- ببینید ، این قدرت تغییر نرخ یادگیری است

- بنابراین ببینید که میزان یادگیری بسیار کوچکتر است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_122___

___preserve_521___

---

___preserve_1254___Section 124: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این از آهنگ غیر رسمی Pytorch من است

- و این یکی در مقایسه با Pytorch کمی به عقب است ، اما ابتدا در برچسب های Y Training عبور می کنیم

- سپس ما قصد داریم با استفاده از ضرر به عقب ، Pytorch را برای ما انجام دهیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_123___

___preserve_521___

---

___preserve_1260___Section 125: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- شما می توانید با نرخ یادگیری ، منظم سازی بازی کنید

- ما می توانیم این کار را انجام دهیم ، البته مدل 3 و سپس در تست X عبور می کنیم

- من واقعاً واقعاً دوست داشتم که ما این کار را انجام دادیم زیرا پس از آن ما مجبور شدیم خطای شکل را در پرواز عیب یابی کنیم زیرا این یکی از رایج ترین موضوعاتی است که شما در یادگیری عمیق با آن روبرو خواهید شد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_124___

___preserve_521___

---

___preserve_1266___Section 126: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- شاید شما نرخ یادگیری را پایین بیاورید زیرا در حال حاضر ما 0 داریم

- از آنجا که به طور بالقوه لایه های بیشتری را اضافه می کنم ، ممکن است تعداد واحدهای پنهان را افزایش دهم ، و سپس اگر نیاز داشتیم برای مدت طولانی تر متناسب باشیم و شاید نرخ یادگیری را به 0 کاهش دهیم

- بنابراین به نظر می رسد نوع داده پیش فرض Pytorch برای اعداد صحیح در 64 است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_125___

___preserve_521___

---

___preserve_1272___Section 127: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته ، شما می توانید توابع فعال سازی پیچیده تر را انجام دهید

- اما استفاده از لایه های Pytorch بسیار ساده تر است زیرا ما در اینجا شبکه های عصبی مانند آجر LEGO می سازیم ، این لایه ها را به نوعی ، شکل یا فرم جمع می کنیم

- و از آنجا که آنها بخشی از Pytorch هستند ، ما می دانیم که آنها آزمایش شده اند و هرچه سریعتر در پشت صحنه محاسبه می شوند و از GPU استفاده می کنند و از مزایای کاملی برخوردار می شوند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_126___

___preserve_521___

---

___preserve_1278___Section 128: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته مشعل

- البته ، شما می توانید آن را هر شماره ای که می خواهید تنظیم کنید ، اما من 42 را دوست دارم

- اوه ، و ما در اینجا به کاما احتیاج داریم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_127___

___preserve_521___

---

___preserve_1284___Section 129: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و ما آن را به Float Torch Dot تبدیل خواهیم کرد زیرا پیش فرض های Numpy را به عنوان Float 64 به یاد داشته باشید ، در حالی که Pytorch Float 32 را دوست دارد

- اوه ، البته ما اشتباهی گرفتیم

- ساختن یک مدل طبقه بندی چند طبقه در Pytorch

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_128___

___preserve_521___

---

___preserve_1290___Section 130: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، البته ، اگر این کار را نکنید ، می توانید نوع اجرا را تغییر دهید ، GPU را در اینجا انتخاب کنید ، که زمان اجرا را دوباره راه اندازی می کند ، باید تمام کدی را که قبل از این سلول است نیز اجرا کنید ، اما من قصد دارم از GPU استفاده کنم

- بنابراین من قصد دارم کلاس خود را در اینجا با مدل Blob Model تماس بگیرم ، و البته این به ارث می برد از NN

- و پس از آن ، البته ، ما ویژگی های خروجی داریم ، که این نیز یک INT است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_129___

___preserve_521___

---

___preserve_1296___Section 131: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و پس از آن ، البته ، ما قصد داریم این را به دستگاه ارسال کنیم

- و البته یک حلقه آموزشی

- از کجا عملکردهای از دست دادن را در Pytorch پیدا می کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_130___

___preserve_521___

---

___preserve_1302___Section 132: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- تسلط یادگیری ماشین نیز وب سایت فوق العاده دیگری است

- اما البته ، در Torch Dot Opt در ماژول ، بهینه سازهای بسیار متفاوتی پیدا خواهید کرد

- و ما نرخ یادگیری را روی 0 تعیین خواهیم کرد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_131___

___preserve_521___

---

___preserve_1308___Section 133: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_132___

___preserve_521___

---

___preserve_1314___section 134: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما مطمئناً ، هنگامی که مدل خود را آموزش دادیم ، این تعداد تغییر می کنند

- 5 ، یک حلقه آموزش و حلقه آزمایش برای یک مدل Pytorch چند طبقه ایجاد کنید

- بیایید اولین حلقه آموزش و آزمایش خود را برای یک مدل Pytorch چند طبقه بسازیم ، و من به شما کمی اشاره می کنم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_133___

___preserve_521___

---

___preserve_1320___Section 135: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_134___

___preserve_521___

---

___preserve_1326___Section 136: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین اگر این کار را دوباره اجرا کنیم و حالا این یکی مدتی طول کشید تا پیدا کنم و می خواهم بدانید که در پشت صحنه ، حتی اگر ، این یک نمایش آشپزی یادگیری ماشین است ، هنوز هم برای عیب یابی کد طول می کشد و شما می خواهید این مسئله را پیدا کنید

- بنابراین ما برچسب های خود را در اینجا به شناور تبدیل کردیم ، که به طور کلی در Pytorch خوب است

- ما به آرامی در اینجا همه خطاها را در یادگیری عمیق کار می کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_135___

___preserve_521___

---

___preserve_1332___Section 137: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما ما همچنین سر خود را در برابر دو مورد متداول در یادگیری ماشین و یادگیری عمیق به طور کلی بلند کردیم

- و به من اعتماد کنید ، شما می خواهید بسیاری از آنها را در تلاش های عمیق یادگیری و یادگیری ماشین خود اجرا کنید

- اما ما می توانیم با ساخت و ارزیابی پیش بینی ها با یک مدل چند طبقه Pytorch ، این موضوع را بیشتر ارزیابی کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_136___

___preserve_521___

---

___preserve_1338___Section 138: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و در واقع ، Pytorch افزودن غیرخطی ها را به مدل ما بسیار آسان می کند ، ما ممکن است آنها را نیز در این زمینه قرار دهیم تا در صورت نیاز به مدل ما از آن استفاده کند و اگر به آن احتیاج نداشته باشد ، خوب ، سلام ، همانطور که قبلاً دیدیم ، یک مدل بسیار خوب را ایجاد می کند اگر در مدل خود بگوییم.

- بنابراین ، فقط این را در خاطر داشته باشید ، و زیبایی Pytorch این است که به ما این امکان را می دهد تا مدل هایی با عملکردهای خطی و غیرخطی کاملاً انعطاف پذیر ایجاد کنیم

- اما ، البته ، چند نمونه سخت وجود داشت که منظور من کمی سخت است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_137___

___preserve_521___

---

___preserve_1344___Section 139: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- Torchmetrics یک کتابخانه مانند Pytorch است

- بنابراین ما عملکرد دقت خودمان را ساخته ایم ، اما زیبایی استفاده از مشعل این است که از کد مانند Pytorch استفاده می کند

- اگر می خواهید به بسیاری از معیارهای Pytorch دسترسی پیدا کنید ، به مشعل ها مراجعه کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_138___

___preserve_521___

---

___preserve_1350___Section 140: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و سپس ، البته ، فوق برنامه

- اما ، البته ، شما فقط می توانید آن را در Learnypytorch پیدا کنید

- و بنابراین اگر می خواهید یک الگوی کد ورزش را دوست داشته باشید ، می توانید به Repo Deep Learning Pytorch بروید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_139___

___preserve_521___

---

___preserve_1356___Section 141: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و بنابراین مدل یادگیری ماشین ما ممکن است پیکسل های یک تصویر را بگیرد و الگوهای مختلفی را که به نظر می رسد یک استیک و همان چیز با یک پیتزا درک می کند ، درک کند

- بنابراین در پشت صحنه ، Nutrify از پیکسل های یک تصویر استفاده می کند و سپس آنها را از طریق یک مدل یادگیری ماشین اجرا می کند و ابتدا آن را طبقه بندی می کند ، چه غذا باشد یا نه غذا

- بنابراین به طور بالقوه ، می توانید یک مدل یادگیری ماشین را برای یافتن این نوع خاص از ماشین طراحی کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_140___

___preserve_521___

---

___preserve_1362___Section 142: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین تسلا از pytorch استفاده می کند

- بنابراین دقیقاً همان کد که ما می نویسیم ، تسلا از کد Pytorch مشابه استفاده می کند ، البته ، آنها کد Pytorch را متناسب با مشکل خود می نویسند

- اما با این وجود ، آنها از کد Pytorch برای آموزش مدل های یادگیری ماشین خود استفاده می کنند که نرم افزار خود رانندگی خود را تأمین می کنند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_141___

___preserve_521___

---

___preserve_1368___Section 143: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و این می تواند ورودی به الگوریتم یادگیری ماشین ما باشد

- و سپس شما ممکن است این الگوریتم یادگیری ماشین را برای خروج از شکل های دقیق مورد نظر خود قرار دهید

- اکنون ممکن است همیشه آن را درست نکند زیرا از این گذشته ، این همان چیزی است که یادگیری ماشین است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_142___

___preserve_521___

---

___preserve_1374___Section 144: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و سپس امیدوارم که الگوریتم یادگیری ماشین شما نوع مناسبی از لباس را که در آن قرار دارد ، خروجی می کند

- بنابراین امیدوارم الگوریتم یادگیری ماشین ما بتواند آنچه را که در این تصاویر اتفاق می افتد تشخیص دهد

- خوب ، از آنجا که شما با نمایش های مختلف بسیاری از داده ها متوقف می شوید ، اما به ویژه داده های تصویر در Pytorch و سایر کتابخانه ها ، بسیاری از کتابخانه ها انتظار دارند کانال های رنگی آخرین بار باشد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_143___

___preserve_521___

---

___preserve_1380___Section 145: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- شما نه تنها یادگیری ساخت آنها را شروع خواهید کرد ، بلکه تازه شروع به یادگیری استفاده از آنها خواهید کرد ، زیرا بعداً در بخش یادگیری انتقال دوره خواهیم دید

- و سپس من قصد دارم این یکی را نامگذاری کنم ، این 03 Pytorch Computer Vision خواهد بود

- بنابراین فقط این برچسب ویدئویی را دارد ، زیرا اگر به اینجا برویم ، اگر به نوت بوک های ویدیویی از repo یادگیری عمیق Pytorch برویم ، نوت بوک های ویدیویی در اینجا ذخیره می شوند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_144___

___preserve_521___

---

___preserve_1386___Section 146: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما قصد داریم این را در بخش پوشش دهیم ، فکر می کنم شش نفر برای یادگیری انتقال است

- بنابراین Pytorch واقعاً برای دید رایانه بسیار مناسب است

- همه پیوندها برای اینها ، به هر حال ، در نسخه کتاب دوره Pytorch Computer Vision است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_145___

___preserve_521___

---

___preserve_1392___Section 147: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین ما کتابخانه های بینایی رایانه را در Pytorch پوشش داده ایم

- بنابراین در آخرین ویدئوی ، ما برخی از کتابخانه های اساسی چشم انداز رایانه را در Pytorch پوشش دادیم

- و پس از آن ، ما مجموعه داده های نقطه داده نقطه استفاده از مشعل را دریافت کرده ایم ، که کلاس مجموعه داده های پایه برای Pytorch و Data Loader است ، که یک پایتون را برای یک مجموعه داده ایجاد می کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_146___

___preserve_521___

---

___preserve_1398___Section 148: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این همان چیزی است که من در مورد چگونگی پیش فرض Pytorch با بسیاری از تحولات به CHW صحبت می کردم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_147___

___preserve_521___

---

___preserve_1404___Section 149: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- با این حال ، برخی دیگر از کتابخانه های یادگیری ماشین ارتفاع ، عرض و سپس کانال های رنگی را ترجیح می دهند

- بنابراین اگر مدل یادگیری ماشین ما نه یا کلاس نه را پیش بینی کرده است ، می توانیم با استفاده از این ویژگی از داده های قطار ، آن را به بوت مچ پا تبدیل کنیم

- و بخشی از تبدیل شدن به یکی از داده ها ، البته بررسی اشکال خروجی ورودی آن است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_148___

___preserve_521___

---

___preserve_1410___Section 150: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون همانطور که گفتم ، این یکی از رایج ترین خطاها در یادگیری ماشین است یک مسئله شکل است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_149___

___preserve_521___

---

___preserve_1416___section 151: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما کل فرضیه مدل های یادگیری ماشین ساختمانی برای انجام این کار می تواند برنامه ای بنویسید که شکل این تصاویر را به خود اختصاص دهد و بفهمد ، یک برنامه مبتنی بر قانون بنویسید ، سلام ، اگر به نظر می رسد مانند یک مستطیل با سگک در وسط باشد ، احتمالاً یک کیسه است

- منظور من این است که شما احتمالاً می توانید بعد از مدتی ، اما من ترجیح می دهم الگوریتم های یادگیری ماشین را بنویسم تا الگوهای و داده ها را بفهمم

- بنابراین در حال حاضر ، داده های ما به صورت مجموعه داده های Pytorch است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_150___

___preserve_521___

---

___preserve_1422___Section 152: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین بله ، یادگیری ماشین در مقیاس بزرگ ، نزول شیب مینی دسته ای ، نزول شیب دسته ای مینی

- و این اصل ، به هر حال ، تهیه یک لودر داده نه تنها برای تصاویر ، بلکه برای متن ، برای صدا ، هر نوع داده ای که با آنها کار می کنید ، یکسان است ، مینی دسته ها شما را دنبال می کنند یا دسته ای از داده ها شما را در کل بسیاری از مشکلات مختلف یادگیری عمیق دنبال می کنند

- به هر حال ، این برنامه درسی اضافی نیز برای شما است ، خواندن این صفحه داده های مشعل داده ها نه داده ها ، زیرا مهم نیست که با یادگیری عمیق یا Pytorch چه مشکلی را پیش می روید ، می خواهید با داده ها کار کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_151___

___preserve_521___

---

___preserve_1428___Section 153: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین این به ما می گوید چند دسته وجود دارد ، دسته هایی از آن که البته اندازه دسته ای است

- البته اکنون تعداد دسته هایی که داریم تغییر می کند اگر اندازه دسته را تغییر دهیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_152___

___preserve_521___

---

___preserve_1434___Section 154: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین من می گویم ویژگی هایی مانند خود تصاویر و دسته برچسب های قطار برچسب های مجموعه داده های ما یا اهداف موجود در اصطلاحات Pytorch است

- هنگام شروع به ساختن یک سری آزمایش های مدل سازی یادگیری ماشین ، بهترین کار برای شروع با یک مدل پایه است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_153___

___preserve_521___

---

___preserve_1440___Section 155: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما می رویم nn flatten ، flatten in pytorch ، چه کاری انجام می دهد

- و البته ، همه nn

- ما می خواهیم یک شکل ورودی داشته باشیم ، که از یک نوع اشاره استفاده خواهیم کرد ، که یک عدد صحیح را به خود اختصاص می دهد زیرا به یاد داشته باشید ، شکل ورودی برای مدل های یادگیری ماشین بسیار مهم است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_154___

___preserve_521___

---

___preserve_1446___Section 156: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- یکی از بزرگترین خطاهایی که در یادگیری ماشین با آن روبرو هستید ، عدم تطابق شکل تانسور متفاوت است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_155___

___preserve_521___

---

___preserve_1452___Section 157: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین این موارد البته با مقادیر تصادفی آغاز می شوند ، اما کل فرضیه یادگیری عمیق و یادگیری ماشین ، عبور داده ها از طریق مدل ما و استفاده از Optimizer ما برای به روزرسانی این مقادیر تصادفی برای نشان دادن بهتر ویژگی های موجود در داده های ما است.

- کل فرضیه آن یا کل سرگرم کننده است ، کل جادوی در پشت یادگیری ماشین این است که می گوید چه ویژگی هایی برای یادگیری است

- خوب ، اگر ما وارد LearnyPytorch شویم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_156___

___preserve_521___

---

___preserve_1458___Section 158: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته ، ما می توانستیم از معیارهای مشعل نیز استفاده کنیم

- البته ، می توانید عملکردهای یاور را سفارشی کنید

- و سپس من قصد دارم نرخ یادگیری را در اینجا تعیین کنم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_157___

___preserve_521___

---

___preserve_1464___Section 159: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و پس از آن البته ، ما می توانیم برای استدلال ها چیزهای بیشتری اضافه کنیم ، اما این یک آستر سریع است

- از آنجا که من از Google Colab Pro استفاده می کنم ، کاملاً غیر ضروری برای این دوره است ، اما من فقط فهمیدم که ارزش آن را برای چقدر از Google Colab استفاده می کنم ، مدت زمان طولانی کار بیکار می شوم ، به این معنی که نوت بوک CoLab من برای مدت زمان طولانی پایدار خواهد ماند

- اما مطمئناً یک شبه قطع خواهد شد ، بنابراین من روی اتصال مجدد کلیک می کنم ، و بعد اگر می خواهم به هر کجا که بودیم برگردم ، زیرا ما برخی از داده ها را از Torchvision بارگیری کردیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_158___

___preserve_521___

---

___preserve_1470___Section 160: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- فوق العاده ، چهار ، ما می خواهیم آنچه را که اتفاق می افتد چاپ کنیم

- شما ممکن است آهنگ غیر رسمی Pytorch Loop را دیده باشید

- و ما می خواهیم همه چیز را برای تفریح ​​به وقت خود برسانیم ، زیرا این همان عملکرد زمانبندی ماست

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_159___

___preserve_521___

---

___preserve_1476___Section 161: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، بعداً ، شما ممکن است فکر کنید ، پس شما ، چگونه ما این حلقه آموزشی را قبلاً عملکردی نکرده ایم

- خوب ، به این دلیل است که ما دوست داریم نوشتن کد Pytorch را تمرین کنیم ، درست است

- بنابراین چاپ به آن نگاه کرد ، و البته می توانید این را با هر آنچه دوست دارید تنظیم کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_160___

___preserve_521___

---

___preserve_1482___Section 162: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و سپس ما از دست دادن آزمون خواهیم گرفت ، البته از دست دادن تست و خواهیم رفت ، ما آن را به چهار مکان اعشاری نیز می رسانیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_161___

___preserve_521___

---

___preserve_1488___Section 163: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بهترین راه برای یادگیری کد Pytorch نوشتن کد Pytorch بیشتر است

- اوه ، البته ما این کار را کردیم

- آنها باید در همان قلمرو معدن باشند ، اما به دلیل تصادفی ذاتی یادگیری ماشین ، حتی اگر بذر دستی را تنظیم کنیم ممکن است کمی متفاوت باشد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_162___

___preserve_521___

---

___preserve_1494___Section 164: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته لودر داده آزمون ، زیرا ما می خواهیم آن را در مجموعه داده های آزمون ارزیابی کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_163___

___preserve_521___

---

___preserve_1500___Section 165: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این Pytorch بررسی خواهد کرد که آیا یک GPU با CUDA موجود است یا خیر

- بنابراین مهم نیست که سیستم ما در حال اجرا است ، Pytorch از آن استفاده می کند

- این یکی دیگر از مزایای استفاده از یک مجموعه داده نسبتاً کوچک است که قبلاً در مجموعه داده های Pytorch ذخیره شده است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_164___

___preserve_521___

---

___preserve_1506___Section 166: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین من از شما خواسته ام که در آخرین فیلم چالش را انجام دهید تا این کار را انجام دهید ، سعی کنید یک الگوی با غیرخطی بسازید ، امیدوارم که این کار را انجام دهید ، زیرا اگر هر چیزی که این دوره باشد ، من سعی می کنم در این دوره به شما منتقل شود ، این است که اگر کارها را امتحان کنید ، اگر این کار را انجام دهید ، عکاسی را امتحان کنید ، و در مورد آن چیزها را امتحان می کنید ، در مورد آن می خواهیم. به یادگیری

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_165___

___preserve_521___

---

___preserve_1512___Section 167: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و نرخ یادگیری ، ما فقط می خواهیم آن را همانند مدل قبلی خود نگه داریم

- عملکرد صحت از یک دوره است که اندازه گیری دقت مدل های ما است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_166___

___preserve_521___

---

___preserve_1518___Section 168: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_167___

___preserve_521___

---

___preserve_1524___Section 169: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- امیدوارم که شما این کار را کرده باشید زیرا این بهترین راه برای تمرین کد Pytorch نوشتن کد Pytorch بیشتر است

- از آنجا که ما به یک مدل احتیاج داریم و به داده ها نیاز داریم ، لودر داده ها البته بار داده های آزمایش در اینجا ، استفاده از نقطه نقطه استفاده از نقطه dot data dot data loader

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_168___

___preserve_521___

---

___preserve_1530___Section 170: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این مهم است زیرا اگر بخواهیم مقداری ایجاد شده در زمینه Context Manager را تطبیق دهیم ، ما باید آن را هنوز هم با آن در آن Manager Manager اصلاح کنیم ، در غیر این صورت Pytorch خطایی را به وجود می آورد

- آیا با استفاده از عملکرد مرحله آموزش ما و یک عملکرد مرحله آزمون ، یک حلقه آموزشی یا یک حلقه بهینه سازی Pytorch ایجاد کرده اید؟

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_169___

___preserve_521___

---

___preserve_1536___Section 171: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما بعداً در این دوره نمونه دیگری از آن را خواهیم دید

- بنابراین این یک چیز بزرگ در مورد یادگیری ماشین است که از تصادفی استفاده می کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_170___

___preserve_521___

---

___preserve_1542___Section 172: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- معمولاً اگر از GPU مانند یک GPU نسبتاً مدرن استفاده می کنید ، در محاسبات ، یادگیری عمیق یا اجرای الگوریتم های یادگیری عمیق سریعتر از پردازنده عمومی شما سریعتر خواهد بود

- و بنابراین اگر می خواهید یک مقاله عالی در مورد چگونگی استفاده بیشتر از GPU های خود داشته باشید ، کمی فنی است ، اما این چیزی است که باید در خاطر داشته باشید که به عنوان یک مهندس یادگیری ماشین پیشرفت می کنید ، چگونه می توانید GPU های خود را به کار بگیرید

- ایجاد یادگیری عمیق همانطور که در GPU شما در حال آشکار شدن است ، زیرا از اصول اول خیلی سریع اجرا می شود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_171___

___preserve_521___

---

___preserve_1548___Section 173: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این به دلیل تصادفی ذاتی یادگیری ماشین و یادگیری عمیق است

- ما یک لایه ورودی داریم ، دقیقاً مانند هر مدل یادگیری عمیق دیگر

- و متوجه خواهید شد که بسیاری از کد ها کاملاً شبیه به کدی است که قبلاً برای سایر مدل های Pytorch می نویسیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_172___

___preserve_521___

---

___preserve_1554___Section 174: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، اگر 100 کلاس داشتیم می توانیم این را تغییر دهیم ، ممکن است این را به 100 تغییر دهیم

- و بله ، البته ، اگر این اولین بار است که این را دیده اید

- اما اساساً ، آنچه اتفاق می افتد یک هسته است که به عنوان فیلتر نیز شناخته می شود ، به مقادیر پیکسل تصویر ما می رود ، زیرا البته آنها در قالب یک تنشور خواهند بود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_173___

___preserve_521___

---

___preserve_1560___Section 175: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- خوب ، یکی ، شما می توانید به مستندات Pytorch ، Pytorch و سپس Com 2d بروید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_174___

___preserve_521___

---

___preserve_1566___Section 176: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، البته ، ما می توانیم بعداً همه این مقادیر را تغییر دهیم ، اما فقط با من تحمل کنیم در حالی که آنها را چگونه تنظیم می کنیم

- و البته ، این می تواند یک Tuple نیز باشد

- و بنابراین به عنوان داده های ما ، این یک روند در همه یادگیری های عمیق است ، در واقع

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_175___

___preserve_521___

---

___preserve_1572___Section 177: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، این در واقع بسیار معمول در یادگیری ماشین است ، یافتن نوعی معماری است که شخصی پیدا کرده است که روی نوعی مشکل کار کند و آن را با کد تکرار کند و ببیند آیا این مسئله روی مشکل شما کار می کند

- و البته ، ما قصد داریم این مدل را به دستگاه ارسال کنیم

- البته ، تایپی

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_176___

___preserve_521___

---

___preserve_1578___Section 178: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، من فوق العاده استوکی هستم زیرا در آخرین ویدئو ، ما اولین شبکه عصبی Convolutional خود را در Pytorch رمزگذاری کردیم

- ما معماری کوچک VGG را از وب سایت CNN توضیح دهنده ، مکان مورد علاقه من برای یادگیری در مورد CNN در مرورگر تکرار کردیم

- خوب ، البته ، ما مستندات و سپس comp2d را داریم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_177___

___preserve_521___

---

___preserve_1584___Section 179: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، البته ، شما می توانید با خواندن اسناد در اینجا ، آن را پیدا کنید

- اکنون ، زیبایی Pytorch این است که همه اینها را در پشت صحنه برای ما انجام می دهد

- این زیبایی یادگیری عمیق این است که می آموزد که چگونه می توان با نگاه به داده های بیشتر ، به خودی خود به بهترین شکل داده های ما را نشان داد.

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_178___

___preserve_521___

---

___preserve_1590___Section 180: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته ما یک خطای شکل می گیریم

- یکی از متداول ترین موضوعات یادگیری ماشین و یادگیری عمیق

- اکنون ، فقط به خاطر داشته باشید که اگر این لایه را اجرا می کنید و سپس Com2d را روی نسخه Pytorch ، یعنی من معتقدم که آنها این را برطرف کرده اند یا آن را در Pytorch تغییر داده اند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_179___

___preserve_521___

---

___preserve_1596___Section 181: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، اگر از نسخه بعدی Pytorch استفاده می کنید ، اگر فقط از یک تانسور تصویر سه بعدی استفاده کنید و از طریق یک لایه Comp عبور کنید ، ممکن است خطایی پیدا نکنید

- و البته ، لایه همگانی ما با اعداد تصادفی فوری می شود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_180___

___preserve_521___

---

___preserve_1602___Section 182: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین همانطور که قبلاً بحث کردیم ، چه شبکه عصبی یادگیری عمیق در تلاش است انجام دهد یا در این حالت ، یک CNN برخی از داده های ورودی را گرفته و می فهمد چه ویژگی هایی به بهترین وجه داده های ورودی را نشان می دهد و آنها را به یک بردار ویژگی تبدیل می کند که خروجی ما خواهد بود

- و اکنون ، البته ، شما می توانید این مقدار را در اینجا سفارشی کنید

- این بخشی از ماهیت آزمایشی یادگیری ماشین است ، اما ما در حال حاضر می خواهیم آن را به دو صورت نگه داریم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_181___

___preserve_521___

---

___preserve_1608___Section 183: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و ما فقط پوشش داده ایم که آن را در طی چند فیلم اخیر شکسته ایم و خودمان را با چند خط کد Pytorch بازسازی کردیم

- بنابراین این فقط نشان می دهد که Pytorch چقدر قدرتمند است و حوزه یادگیری عمیق تا چه حد رسیده است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_182___

___preserve_521___

---

___preserve_1614___Section 184: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- باز هم ، ما در حال گذراندن هر سه موضوع اصلی در یادگیری عمیق هستیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_183___

___preserve_521___

---

___preserve_1620___Section 185: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و اگر ببینیم کجا در گردش کار Pytorch هستیم ، داده های خود را آماده کرده ایم

- و ما از نرخ یادگیری 0 استفاده خواهیم کرد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_184___

___preserve_521___

---

___preserve_1626___Section 186: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و پس از آن البته ، دستگاه با دستگاه برابر است

- اوه ، البته

- بنابراین می توانید تمام کارکردهایی را که در پشت صحنه از Pytorch خوانده می شوند ، ببینید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_185___

___preserve_521___

---

___preserve_1632___Section 187: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و پس از آن ، اوه ، ببخشید ، تایپی ، عملکرد از دست دادن ما البته عملکرد از دست دادن ما خواهد بود

- اما بیایید اطمینان حاصل کنیم که با مقایسه ، این بخش مهم دیگری از آزمایشات یادگیری ماشین مقایسه نتایج در آزمایشات شما است

- مطمئناً می توانیم اندازه هسته را تغییر دهیم ، بالشتک را تغییر دهیم ، استخر حداکثر را تغییر دهیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_186___

___preserve_521___

---

___preserve_1638___Section 188: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و کل نکته ساخت یک مدل یادگیری ماشین در داده های دید رایانه این است که بتوانید پیش بینی ها را تجسم کنید

- این یکی از مراحل مورد علاقه من پس از آموزش یک مدل یادگیری ماشین است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_187___

___preserve_521___

---

___preserve_1644___Section 189: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این یک پنج است ، که البته نام کلاس در آن فهرست بندی خواهد شد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_188___

___preserve_521___

---

___preserve_1650___Section 190: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- از آنجا که ما نه نمونه تصادفی داریم ، شما می توانید این را به هر حال بسیاری از شما تغییر دهید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_189___

___preserve_521___

---

___preserve_1656___Section 191: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ببینید که چگونه ، برای من ، من بسیار قدردانی می کنم ، مانند ، من ترجیح می دهم که شماره ها را در یک صفحه تجسم به نظر برسانم ، اما چیزی وجود ندارد ، هیچ چیز کاملاً شبیه به تجسم پیش بینی های مدل های یادگیری ماشین شما نیست ، به خصوص وقتی که درست شود

- ما در ارزیابی مدل یادگیری ماشین خود به یک نکته بسیار هیجان انگیز هستیم

- و اگر به یاد بیاورید ، اگر به بخش دوم Pytorch Lone برگردیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_190___

___preserve_521___

---

___preserve_1662___Section 192: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین به یاد بیاورید که معیارهای مشعل ما قبلاً در این مورد لمس کرده ایم ، معیارهای مشعل عالی برای یک دسته از معیارهای ارزیابی مدل های یادگیری ماشین در طعم Pytorch است

- و آیا متوجه می شوید که چگونه این کاملاً شبیه به مستندات Pytorch است

- خوب ، این نکته زیبا در مورد معیارهای مشعل این است که با توجه به Pytorch ایجاد شده است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_191___

___preserve_521___

---

___preserve_1668___Section 193: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_192___

___preserve_521___

---

___preserve_1674___Section 194: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، ما می توانیم اهداف را بدست آوریم ، که برچسب ها است

- Pytorch اهداف برچسب را می خواند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_193___

___preserve_521___

---

___preserve_1680___Section 195: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته برعکس

- و شما البته بیشتر در معیارهای مشعل دارید

- بنابراین اگر از قسمت های دیگر دوره عبور کرده اید ، قطعاً دارید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_194___

___preserve_521___

---

___preserve_1686___section 196: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته ، شما می توانید نام این نام را سفارشی کنید ، جایی که آن را ذخیره می کنید ، et cetera ، et cetera

- همچنین پیوندی به مستندات Pytorch وجود دارد که به شدت توصیه می کند

- و البته ، ما قصد داریم آن را در همان مجموعه داده های آزمون که ما از Loader Test Data Loader استفاده کرده ایم ارزیابی کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_195___

___preserve_521___

---

___preserve_1692___section 197: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بیایید در ویدیوی بعدی ، فکر می کنم این کد کافی برای این بخش ، بخش سه ، Pytorch Computer Vision است

- ببینید چقدر کد Pytorch Computer Computer را با هم نوشتیم

- ما کتابخانه های دید رایانه و Pytorch را بررسی کردیم ، اصلی ترین دید مشعل

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_196___

___preserve_521___

---

___preserve_1698___Section 198: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- با کلیک بر روی گزینه های مختلف در کتابخانه Pytorch Vision ، Torch Vision ، به جستجوی متداول ترین شبکه های عصبی Convolutional در کتابخانه مدل Torch Vision بپردازید ، و سپس برای تعداد بیشتری از مدلهای رایانه ای Pytorch از پیش آموزش داده شده ، و اگر به دید رایانه عمیق تر می شوید ، احتمالاً می خواهید به کتابخانه مدل های تصویر مشعل بپردازید ، در غیر این صورت 10 ، اما من می خواهم به عنوان برنامه درسی اضافی را ترک کنم.

- دوباره ، آن را در Pytorch Learn است

- اکنون این در repo یادگیری عمیق Pytorch است ، تمرینات اضافی شماره سه

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_197___

___preserve_521___

---

___preserve_1704___Section 199: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و بنابراین من می خواهم کتابخانه های دامنه Pytorch را برجسته کنم

- و البته ، پاداش داده های مشعل است

- همانطور که گفتم ، ما می خواهیم این تصاویر را در Pytorch بارگذاری کنیم تا بتوانیم یک مدل بسازیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_198___

___preserve_521___

---

___preserve_1710___Section 200: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما قصد داریم یک مجموعه داده سفارشی را با Pytorch بدست آوریم

- اما من دوست دارم یادگیری ماشین را به عنوان کمی یک هنر درمان کنم ، بنابراین ما قصد داریم کد زیادی را تهیه کنیم

- به نمایشگاه آشپزی Pytorch خوش آمدید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_199___

___preserve_521___

---

___preserve_1716___section 201: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما اکنون ، اوه ، و همچنین ، اگر به وام Pytorch بروید

- اما اکنون ما قصد داریم کد را بنویسیم ، زیرا این مجموعه داده ، نسخه کوچکتر که من ایجاد کرده ام در Repo Learning Deep Deep Repo ، تحت داده ها است

- و این ، البته ، به جایی که مجموعه داده های شما در کجا زندگی می کند بستگی دارد ، کاری که می خواهید انجام دهید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_200___

___preserve_521___

---

___preserve_1722___Section 202: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و سپس کل پیش فرض این بخش بارگیری این داده های فقط تصاویر در Pytorch خواهد بود تا بتوانیم یک مدل دید رایانه ای را روی آن بسازیم

- چه کسی می داند که داده های شما در هر کجا نیست ، اما می خواهید کد بنویسید تا آن را از اینجا به Pytorch بارگذاری کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_201___

___preserve_521___

---

___preserve_1728___Section 203: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- D Burke Pytorch Learning عمیق ، ما به جای حباب خام داریم

- ما یک مجموعه داده هدف را بارگذاری می کنیم و سپس نوشتن کد برای تبدیل هر نوع فرمی که مجموعه داده ها در آن برای Pytorch قرار دارد ، تبدیل می شود

- در آخرین ویدئو ، ما برای بارگیری یک مجموعه داده هدف ، مجموعه داده های سفارشی خودمان از فهرست داده های یادگیری عمیق Pytorch ، برخی از کد ها را نوشتیم.

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_202___

___preserve_521___

---

___preserve_1734___Section 204: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما فرضیه باقی مانده است ، ما قصد داریم کد را بنویسیم تا داده های خود را در اینجا به تنش برای استفاده با Pytorch بدست آوریم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_203___

___preserve_521___

---

___preserve_1740___Section 205: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین بیایید تصادفی را وارد کنیم ، زیرا یادگیری ماشین همه در مورد مهار قدرت تصادفی است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_204___

___preserve_521___

---

___preserve_1746___Section 206: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما همچنین می توانیم تصویر را با Pytorch در اینجا باز کنیم

- اوه ، ما در راه ایجاد مجموعه داده های سفارشی Pytorch خودمان خوب هستیم

- زیرا یکی از خطاهای اصلی در یادگیری ماشین و یادگیری عمیق چیست

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_205___

___preserve_521___

---

___preserve_1752___Section 207: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما اگر ما کانال های رنگی را در کانال های رنگی شروع قرار دهیم ، به طور پیش فرض است

- اما در حال حاضر Pytorch ابتدا به کانال های رنگی پیش فرض می کند

- اما اکنون ما می خواهیم کد بنویسیم تا همه این تصاویر را به زمان Pytorch تبدیل کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_206___

___preserve_521___

---

___preserve_1758___Section 208: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- این در اصل کد Pytorch شما را به یک اسکریپت پایتون تبدیل می کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_207___

___preserve_521___

---

___preserve_1764___Section 209: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- ما می خواهیم تصاویر خود را به عنوان Tenses Pytorch

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_208___

___preserve_521___

---

___preserve_1770___Section 210: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_209___

___preserve_521___

---

___preserve_1776___Section 211: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- Pytorch دوست دارد از Target استفاده کند ، من دوست دارم از برچسب استفاده کنم ، اما اشکالی ندارد

- بنابراین این تبدیل تصاویر ما را اجرا می کند ، هر تصویر از این پوشه ها بارگیری می شود ، از طریق این دگرگونی که ما در اینجا ایجاد کرده ایم ، آنها را تغییر اندازه می دهیم ، به طور تصادفی آنها را روی افقی می چرخانیم ، و سپس آنها را به تنش تبدیل می کنیم ، این دقیقاً همان چیزی است که ما آنها را برای مدل های Pytorch خود می خواهیم

- این یکی از مزایای استفاده از یک لودر داده از پیش ساخته Pytorch است ، یا لودر مجموعه داده این است که با چند ویژگی نسبتاً مناسب همراه است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_210___

___preserve_521___

---

___preserve_1782___Section 212: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و پس از آن ، اگر می خواهید ویژگی های بیشتری را کشف کنید ، می توانید Data Data Dot را ببینید ، و سپس ما چند مورد دیگر ، توابع ، تصاویر ، لودر ، نمونه ها ، اهداف را به دست آورده ایم

- بنابراین این بدان معنی است که داده های ما ، مجموعه داده های سفارشی ما ، این بسیار هیجان انگیز است ، اکنون برای استفاده با یک مدل Pytorch سازگار است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_211___

___preserve_521___

---

___preserve_1788___Section 213: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- در پیتور

- بنابراین ما هنوز در اینجا به گردش کار Pytorch خود پایبند هستیم

- بنابراین اگر سعی کردیم 100000 تصویر را در آن بارگذاری کنیم ، در حالی که با یک مدل Pytorch نیز روی آنها محاسبه می کنیم ، به طور بالقوه می خواهیم حافظه را از بین ببریم و به موضوعاتی بپردازیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_212___

___preserve_521___

---

___preserve_1794___Section 214: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اگر آن را روی سخت افزار یادگیری عمیق اختصاصی اجرا می کنید ، حتی ممکن است حتی بیشتر نیز داشته باشید

- اکنون ، البته ، این تغییر می کند اگر ما تنظیم کنیم ، اوه ، ما حتی این را به اندازه دسته پارامتر دسته ای تنظیم نکردیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_213___

___preserve_521___

---

___preserve_1800___Section 215: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین کل هدف این ویدیو شروع به نوشتن یک تابع یا کلاس است که قادر به بارگیری داده ها از اینجا در قالب Tensor است ، قادر به استفاده از کلاس Loader Data Pytorch است ، مانند ما در اینجا انجام داده ایم

- اما در سطح پایه Pytorch مشعل است

- و یک طرفدار دیگر این است که شما محدود به توابع مجموعه داده های پیش ساخته Pytorch نیستید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_214___

___preserve_521___

---

___preserve_1806___Section 216: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته ، شما می توانید این موضوع را در مستندات پایتون جستجو کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_215___

___preserve_521___

---

___preserve_1812___section 217: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_216___

___preserve_521___

---

___preserve_1818___Section 218: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، البته ، این ویژگی ها بسته به مجموعه داده شما متفاوت خواهند بود

- این بسیار هیجان انگیز است ، زیرا وقتی با مجموعه داده های از پیش ساخته کار می کنید ، در یادگیری ماشین بسیار جالب است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_217___

___preserve_521___

---

___preserve_1824___Section 219: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_218___

___preserve_521___

---

___preserve_1830___Section 220: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین ما قصد داریم در اینجا یک تحول ایجاد کنیم تا بتوانیم تصاویر خود را به صورت jpeg jpeg به تنش تبدیل کنیم ، زیرا این کل هدف از وارد کردن داده ها به Pytorch است

- اما اکنون ما از تبدیل برای تبدیل از Pytorch یا Torch Vision DOT استفاده می کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_219___

___preserve_521___

---

___preserve_1836___Section 221: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و بنابراین ، از این رو از این طریق این است که هر داده ای که داشته باشید ، Pytorch یک کلاس مجموعه داده پایه را به شما می دهد تا از آن استفاده کنید

- بنابراین این به شما کمک می کند تا با مجموعه داده های سفارشی خود در Pytorch کار کنید

- ما از نظر تحلیلی مشاهده کرده ایم که مجموعه داده های سفارشی ما کاملاً شبیه به Pytorch اصلی ، Torch Vision Dot Data مجموعه داده های پوشه تصویر است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_220___

___preserve_521___

---

___preserve_1842___Section 222: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین می توانید چیز زیبایی را در مورد Python و Pytorch سفارشی کنید ، زیرا می توانید این توابع نمایشگر را به هر شکلی که می بینید سفارشی کنید

- بنابراین اگر به طور پیش فرض به یاد بیاوریم ، Pytorch قصد دارد ابعاد تصویر ما را به چه کانال های رنگی تبدیل کند ، با این حال ، Matplotlib کانال های رنگی را به آخرین بار ترجیح می دهد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_221___

___preserve_521___

---

___preserve_1848___Section 223: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین این پوشه تصویر داخلی Pytorch است

- و ما هنوز هم می توانیم از آن با Loader Data Pytorch استفاده کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_222___

___preserve_521___

---

___preserve_1854___Section 224: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، کارگران بی حسی ، ما همچنین می توانیم این کارگران بی حسی را برابر با تعداد CPU صفر یا OS DOT تنظیم کنیم

- و بنابراین این دقیقاً همان کاری است که Pytorch با مجموعه داده های Dot Data Data Data Data Data Data Date Dat

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_223___

___preserve_521___

---

___preserve_1860___Section 225: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و این موردی است که اخیراً برای آموزش مدل های تصویر Vision Pytorch Torch به حالت سطح هنر استفاده می شود

- بنابراین بیایید نگاهی به یک نوع خاص از افزایش داده ها ، که برای آموزش مدل های Vision Pytorch برای حالت سطح ART استفاده می شود ، نگاهی بیندازیم

- بنابراین این یک پست وبلاگ اخیر توسط تیم Pytorch است ، چگونه می توان Models of the Art را آموزش داد ، این همان کاری است که ما می خواهیم انجام دهیم ، وضعیت هنر به معنای بهترین در تجارت است ، در غیر این صورت با نام سودا شناخته می شود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_224___

___preserve_521___

---

___preserve_1866___Section 226: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین مجموعه ای از موارد مختلف مانند بهینه سازی نرخ یادگیری ، آموزش برای مدت طولانی وجود دارد

- این از کتابخانه Transforms Torch Torch Pytorch است

- و از آن برای آموزش جدید برای آموزش جدیدترین حالت مدل های Art Vision در کتابخانه های Pytorch Torch Vision Models یا مخازن استفاده شده است.

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_225___

___preserve_521___

---

___preserve_1872___Section 227: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، شما می توانید کمی بیشتر در اسناد و مدارک بخوانید ، یا با عرض پوزش در مقاله اینجا

-من به تازگی تقویت بی اهمیت را برجسته کرده ام زیرا این همان چیزی است که تیم Pytorch در جدیدترین پست وبلاگ خود برای دستور العمل آموزشی خود برای آموزش مدلهای پیشرفته و پیشرفته از آن استفاده کرده است

-در آخرین ویدئو ، ما پوشش دادیم که چگونه تیم Pytorch از Trivial Aghment Wide استفاده کرد ، که آخرین پیشرفته ترین هنر در افزایش داده ها در زمان ضبط این فیلم برای آموزش آخرین مدل های پیشرفته رایانه ای رایانه ای است که در Torch Vision هستند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_226___

___preserve_521___

---

___preserve_1878___Section 228: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین آن را به گونه ای ببندید تا بتوانیم از آن با یک مدل Pytorch استفاده کنیم

- و من می دانم که قبلاً این کار را قبلاً کدگذاری کرده ایم ، اما خوب است که ببینیم ساخت مدل های Pytorch از ابتدا چیست ، کلاس مدل VGG کوچک ایجاد می کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_227___

___preserve_521___

---

___preserve_1884___Section 229: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- کاملاً معمول در یادگیری ماشین برای یافتن مدلی که برای مشکلی شبیه به شما کار می کند و سپس آن را کپی کرده و آن را بر روی مشکل خود امتحان کنید

- البته ، ما می توانیم با جستجوی فرمول برای شکل ورودی و خروجی لایه های حلقوی آنها را با دست محاسبه کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_228___

___preserve_521___

---

___preserve_1890___Section 230: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون این موضوع فراتر از محدوده این دوره است ، اساساً ، تمام آنچه شما باید بدانید این است که همجوشی اپراتور در پشت صحنه سرعت می گیرد که چگونه GPU شما محاسبات را انجام می دهد

- این فیوژن اپراتور است ، مهمترین بهینه سازی در کامپایلرهای یادگیری عمیق

- بنابراین من این را پیوند خواهم داد ، و یادگیری عمیق از اصول اول توسط هوراس هار ، یک پست عالی وبلاگ که واقعاً دوست دارم ، درست می شود

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_229___

___preserve_521___

---

___preserve_1896___Section 231: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین این در CPU ، دسته تصویر است ، در حالی که مدل ما البته در دستگاه هدف است

- البته در دستگاه CUDA است

- البته ما این کار را می کنیم ، زیرا اکنون شکل های مختلفی پیدا کرده ایم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_230___

___preserve_521___

---

___preserve_1902___Section 232: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما من می خواهم یکی از بسته های مورد علاقه خود را برای یافتن اطلاعات از مدل Pytorch به شما معرفی کنم

- و البته ، شما می توانید تقریباً از هر مدل Pytorch این نوع خروجی را دریافت کنید

- و البته ، اگر بخواهیم ، می توانیم این مقادیر را در اینجا تغییر دهیم ، 24 تا 24

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_231___

___preserve_521___

---

___preserve_1908___Section 233: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- باید با بیشتر مدل های Pytorch شما کار کند

- و البته ، برای مرحله قطار و برای مرحله آزمون ، هر یک از آنها به ترتیب قصد دارند یک لودر داده آموزش را بگیرند

- البته ، ما می خواهیم که آنها به ترتیب لودر داده خود را گرفته شوند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_232___

___preserve_521___

---

___preserve_1914___Section 234: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- خوب ، البته ، ما پاس رو به جلو ، پاس رو به جلو را انجام می دهیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_233___

___preserve_521___

---

___preserve_1920___Section 235: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- در آخرین ویدئوی ، من این چالش را برای ترکیب عملکرد مرحله قطار ما و همچنین عملکرد مرحله تست ما در عملکرد خود به شما صادر کردم تا بتوانیم فقط با یک عملکرد که هر دو را صدا می کند تماس بگیریم و یک مدل را آموزش دهیم و آن را ارزیابی کنیم ، البته

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_234___

___preserve_521___

---

___preserve_1926___Section 236: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، می توانید آن را کدگذاری کنید

- بنابراین لودر داده در اینجا البته لودر داده قطار خواهد بود

- و عملکرد قطار البته قصد دارد عملکرد مرحله قطار و عملکرد مرحله تست ما را فراخوانی کند

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_235___

___preserve_521___

---

___preserve_1932___Section 237: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، اگر ما به گردش کار Pytorch مراجعه کنیم ، من در آخرین فیلم چالش را برای شما صادر کردم تا یک عملکرد ضرر و بهینه ساز ایجاد کنید

- و پس از آن ، البته ، ما قصد داریم مدل هدف را به دستگاه هدف ارسال کنیم

- اکنون ، البته ، بهینه ساز یکی از پارامترهای هایپر است که می توانید برای مدل خود تنظیم کنید ، و یک پارامتر Hyper یک ارزشی است که می توانید خودتان تنظیم کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_236___

___preserve_521___

---

___preserve_1938___Section 238: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، اگر بخواهیم می توانستیم مدل خود را طولانی تر آموزش دهیم

- بنابراین به همین دلیل ما فقط برای پنج نفر آموزش می دهیم ، شاید بعداً شما 10 ، 20 را آموزش دهید ، میزان یادگیری را تغییر دهید ، یک دسته کامل از کارهای مختلف را انجام دهید

- البته ، ما دوست داریم این شماره بالاتر برود ، و شاید در صورت آموزش مدت طولانی تر باشد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_237___

___preserve_521___

---

___preserve_1944___Section 239: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، اگر برای دوره های بیشتر آموزش ببینیم ، این لیست ها طولانی تر خواهند بود

- و پس از آن ، اگر دقت بیشتر شود ، ضرر کاهش می یابد

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_238___

___preserve_521___

---

___preserve_1950___Section 240: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین این راهنمای تست و اشکال زدایی و یادگیری ماشین است

- بنابراین در مورد ما ، اگر به منحنی های ضرر خود برگردیم ، البته ما می خواهیم این امر پایین تر باشد و می خواهیم دقت ما بالاتر باشد

- و بنابراین دو مورد از بزرگترین مشکلات در یادگیری ماشین ، تلاش برای کمبود آن است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_239___

___preserve_521___

---

___preserve_1956___Section 241: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- و البته ، ترکیبات بیشتری از اینها وجود دارد

- آنها همیشه مانند بسیاری از موارد در یادگیری ماشین کار نمی کنند

- از یادگیری انتقال استفاده کنید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_240___

___preserve_521___

---

___preserve_1962___Section 242: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- می توانید دوباره نرخ یادگیری را تغییر دهید

- شاید میزان یادگیری شما برای شروع خیلی زیاد باشد و مدل شما خیلی خوب یاد نمی گیرد

- بنابراین شما می توانید دوباره نرخ یادگیری را تنظیم کنید ، دقیقاً همانطور که با دستیابی به آن سکه در پشت یک نیمکت صحبت کردیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_241___

___preserve_521___

---

___preserve_1968___Section 243: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین این مطابق با گردش کار Pytorch ما است ، یک مدل را امتحان می کند و یک مدل دیگر را امتحان می کند و یکی دیگر را امتحان می کند ، بنابراین و دوباره دوباره

-ما قصد داریم از افزایش داده های تقویت کننده Trivial استفاده کنیم ، آموزش ایجاد آموزش را ایجاد کنیم ، که همانطور که در یک فیلم قبلی دیدیم ، آنچه که تیم Pytorch اخیراً برای آموزش مدلهای پیشرفته رایانه ای رایانه خود استفاده کرده است.

- مدل های ما در حال یادگیری هر الگوی قابل تعمیم در مجموعه داده های آزمون نیستند ، به همین دلیل ما افزایش داده های خود را بر روی مجموعه داده های آموزش متمرکز می کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_242___

___preserve_521___

---

___preserve_1974___Section 244: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اگر واقعاً می خواستیم با افزایش تعداد دوره ها ، این مدل را طولانی تر آموزش دهیم

- عملکرد از دست دادن اغلب در Pytorch معیار نامیده می شود

- ما قصد داریم نرخ یادگیری را روی صفر صفر قرار دهیم ، که پیش فرض برای بهینه ساز اتم در Pytorch است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_243___

___preserve_521___

---

___preserve_1980___Section 245: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین اگر به بخش چهارم Learnypytorch برگردیم

- آیا می توانیم از یادگیری انتقال استفاده کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_244___

___preserve_521___

---

___preserve_1986___Section 246: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- سپس ، البته ، ابزارهایی برای انجام این کار وجود دارد ، مانند Pytorch Plus tensorboard

- بنابراین من به این پیوند ، Pytorch Tensorboard پیوند می دهم

- ما قصد داریم این را در بخش بعدی دوره ببینیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_245___

___preserve_521___

---

___preserve_1992___Section 247: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- سپس ما می خواستیم یک تصویر بارگذاری کنیم و آن را با مدل Pytorch طبقه بندی کنیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_246___

___preserve_521___

---

___preserve_1998___Section 248: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ما قصد داریم به یکی از جالب ترین بخش های یادگیری عمیق برویم

- بنابراین ما می توانیم با استفاده از مدل Pytorch آموزش دیده خود ، یک فرآیند مشابه را تکرار کنیم ، یا آن

- بنابراین این تصویر در دوره GitHub است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_247___

___preserve_521___

---

___preserve_2004___Section 249: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بیایید بفهمیم که چگونه می توانیم تصویر ، تصویر سفارشی خود ، تصویر مفرد ما را به شکل تنش بدست آوریم ، در یک تصویر سفارشی با Pytorch بارگیری می کنیم و بخش دیگری را در اینجا ایجاد می کنیم

- این فقط به شما کمک می کند تا خود را با تمام عملکردهای کتابخانه های دامنه Pytorch آشنا کنید

- ما می توانیم با استفاده از آن تصویری را در Pytorch بخوانیم و با آن برویم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_248___

___preserve_521___

---

___preserve_2010___Section 250: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اکنون ، من می خواهم فقط چیزی راجع به اهمیت انواع مختلف داده ها و شکل ها و چه چیزی و دستگاه ها ، سه مورد از بزرگترین خطا در یادگیری عمیق برجسته کنم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_249___

___preserve_521___

---

___preserve_2016___Section 251: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- البته ، این همان چیزی است که ما در اینجا فراموش کردیم

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___ preserve_250___

___preserve_521___

---

___preserve_2022___Section 252: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین مطمئن شوید که از سه مشعل پای بزرگ و خطای یادگیری عمیق مراقبت می کنید

- بنابراین اولین بعد این تنش براکت های داخلی خواهد بود.

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_251___

___preserve_521___

---

___preserve_2028___Section 253: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_252___

___preserve_521___

---

___preserve_2034___Section 254: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- یکی از جالب ترین کارهایی که باید هنگام ساخت مدل های یادگیری عمیق انجام دهید

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_253___

___preserve_521___

---

___preserve_2040___Section 255: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- اما البته ، چند روش عادلانه وجود دارد که ما می توانیم عملکرد مدل های خود را بهبود بخشیم

- و سپس بسیاری از یادگیری ماشین با تعادل بین بیش از حد و زیربنایی برخورد می کنند

- بسیاری از تحقیقات و یادگیری ماشین در واقع به این تعادل اختصاص داده شده است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_254___

___preserve_521___

---

___preserve_2046___Section 256: موضوعات اصلی ___preserve_517___

** مباحث کلیدی: **

- بنابراین این من هستم که کل چیز را پخش می کنم ، و یک دسته از کد Pytorch را می نویسم

- اوه ، به هر حال ، این در برگه تمرینات اضافی Pytorch Repo Deep Learning است

- بنابراین مجموعه داده های سفارشی Pytorch بوده است

___preserve_518___

___ preserve_519 ___ 📄 برای مشاهده محتوای دقیق ___preserve_520___ کلیک کنید

___preserve_255___

___preserve_521___

---

___preserve_2052___

# دسترسی به لینک دوره

📚 به مطالب کامل علوم داده و یادگیری ماشین دسترسی به مواد: دسترسی به مواد:

___preserve_515___

---

** توجه: ** این لینک دسترسی به مواد کامل Bootcamp را فراهم می کند. ظرف دوره شامل موارد زیر است:

- اثرات شناور مدرن

- طراحی پاسخگو

- ادغام نماد SVG

- سایه های به سبک مواد

- انیمیشن های صاف

*با متغیرهای CSS برای سفارشی سازی موضوع آسان طراحی شده است*